{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "642183e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./shortanswer_ratings_cache.json\", 'r', encoding='utf-8') as f:\n",
    "    sa_cache = json.load(f)\n",
    "\n",
    "sa_qdict = {}\n",
    "for qaid, qdata in sa_cache.items():\n",
    "    qid = qdata['qid']\n",
    "    if qid not in sa_qdict:\n",
    "        sa_qdict[qid] = {\"question\": qdata['question'], \"correct_answer\": qdata['correct_answer']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1df0507",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./compiled_results_sqa/grok-3-latest_phase1_compiled.json\", 'r', encoding='utf-8') as f:\n",
    "    claude_results = json.load(f)\n",
    "\n",
    "for qid in claude_results[\"results\"].keys():\n",
    "    if qid not in sa_qdict:\n",
    "        print(f\"Warning: {qid} not found in sa_qdict\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa37f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provider: Anthropic\n",
      "Processing question 499, ID: sqa_test_2469\n",
      "In model_call, provider=Anthropic, attempt=1\n",
      "Processing question 500, ID: sqa_test_1573\n",
      "In model_call, provider=Anthropic, attempt=1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from base_game_class import BaseGameClass\n",
    "\n",
    "judge_model = \"claude-opus-4-20250514\"\n",
    "sa_to_mc_file = \"./SimpleMC.jsonl\"\n",
    "\n",
    "fout = open(sa_to_mc_file, 'a', encoding='utf-8')\n",
    "\n",
    "judge = BaseGameClass(subject_id=None, subject_name=judge_model, is_human_player=False, log_dir=None)\n",
    "sysprompt=\"\"\n",
    "prompt = \"\"\"I need your help turning a short-answer quiz into a multiple-choice quiz. I'm going to show you a question and its correct answer, and I want you to generate three distractors. \n",
    "Each distractor should be a plausible answer that is NOT the correct answer. Each should be the same \\\"type\\\" of answer as the correct answer (e.g., date, person name, number, etc), and follow the format of the correct answer.\n",
    "Output each distractor a separate line, and do not include any other text. Your entire response should be just the distractors, one per line.\n",
    "Here is the question and correct answer:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Correct Answer: {correct_answer}\n",
    "\n",
    "Distractors:\n",
    "\"\"\"\n",
    "for ctr, (qid, qdata) in enumerate(sa_qdict.items()):\n",
    "    question = qdata['question']\n",
    "    correct_answer = qdata['correct_answer'].strip()\n",
    "    print(f\"Processing question {ctr+1}, ID: {qid}\")\n",
    "    \n",
    "    while True:\n",
    "        resp, _, _ = judge._get_llm_answer(options=None, q_text=prompt.format(question=question, correct_answer=correct_answer), message_history=[], keep_appending=False, setup_text=sysprompt, MAX_TOKENS=None, temp=1.0)\n",
    "        ans_list = re.split(r'\\n+', resp) if resp else []\n",
    "        ans_set = set([a.strip().upper() for a in ans_list] + [correct_answer.upper()]) \n",
    "        if len(ans_set) == 4 and all(ans.strip() for ans in ans_list):\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Invalid response format for question {qid}. Retrying...\")\n",
    "    fout.write(json.dumps({\"qid\": qid, \"question\": question, \"correct_answer\": correct_answer, \"distractors\": ans_list}, ensure_ascii=False) + \"\\n\")\n",
    "    fout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1194fad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load SimpleMC...\n",
      "Dataset loaded successfully.\n",
      "Attempting to load SimpleQA (test split)...\n",
      "Dataset loaded successfully.\n",
      "Formatting 4326 questions...\n",
      "Successfully formatted 4326 unique questions from SimpleQA.\n",
      "Formatting 500 questions...\n",
      "Successfully formatted 500 unique questions from SimpleMC.\n"
     ]
    }
   ],
   "source": [
    "from load_and_format_datasets import load_and_format_dataset\n",
    "qs=load_and_format_dataset(\"SimpleMC\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90ef3eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found question at index 36: {'id': 'sqa_test_479', 'question': 'Which of the three Olympic fencing weapons was the last one to transition to using electrical equipment?', 'options': {'A': 'Sabre', 'B': 'Foil', 'C': 'Rapier', 'D': 'Épée'}, 'correct_answer': 'A', 'answer_type': 'Person', 'topic': 'Politics'}\n"
     ]
    }
   ],
   "source": [
    "for i, q in enumerate(qs):\n",
    "    if q['question'] == 'Which of the three Olympic fencing weapons was the last one to transition to using electrical equipment?':\n",
    "        print(f\"Found question at index {i}: {q}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "629ade74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load SimpleQA (test split)...\n",
      "Dataset loaded successfully.\n",
      "Formatting 4326 questions...\n",
      "Successfully formatted 4326 unique questions from SimpleQA.\n"
     ]
    }
   ],
   "source": [
    "sqa=load_and_format_dataset(\"SimpleQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f6e3d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found question at index 1500: {'id': 'sqa_test_789', 'question': 'Which of the three Olympic fencing weapons was the last one to transition to using electrical equipment?', 'correct_answer': 'Sabre', 'answer_type': 'Other', 'topic': 'Sports'}\n"
     ]
    }
   ],
   "source": [
    "for i, q in enumerate(sqa):\n",
    "    if q['question'] == 'Which of the three Olympic fencing weapons was the last one to transition to using electrical equipment?':\n",
    "        print(f\"Found question at index {i}: {q}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d504b08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import json\n",
    "def text_to_id(text):\n",
    "    return \"sqa_test_\" + hashlib.sha256(text.encode('utf-8')).hexdigest()\n",
    "\n",
    "## replace every \"id\" field value in \"phase1_questions\" and \"phase2_questions\" with a new id based on the \"question\" text, and every \"question_id\" in \"results\" with a new id based on the \"question_text\" text\n",
    "for filename in os.listdir(\"delegate_game_logs\"):\n",
    "   if \"_Simple\" in filename and filename.endswith(\"_game_data.json\"):\n",
    "       fname = os.path.join(\"delegate_game_logs\", filename)\n",
    "       with open(fname, 'r', encoding='utf-8') as f:\n",
    "           game_data = json.load(f)\n",
    "       for q in game_data['phase1_questions']:\n",
    "           q['id'] = text_to_id(q['question'])\n",
    "       for q in game_data['phase2_questions']:\n",
    "           q['id'] = text_to_id(q['question'])\n",
    "       for q in game_data['results']:\n",
    "           q['question_id'] = text_to_id(q['question_text'])\n",
    "       with open(fname, 'w', encoding='utf-8') as f:\n",
    "           json.dump(game_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f006a6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "targ_dir = \"compiled_results_sqa\"\n",
    "for filename in os.listdir(targ_dir):\n",
    "   if not \"claude-3-5-sonnet-20241022_phase1_compiled.json\" in filename:\n",
    "       continue\n",
    "   if filename.endswith(\".json\"):\n",
    "    fname = os.path.join(targ_dir, filename)\n",
    "    with open(fname, 'r', encoding='utf-8') as f:\n",
    "        game_data = json.load(f)\n",
    "    \n",
    "    # Create new results dict with updated keys\n",
    "    new_results = {}\n",
    "    for old_id, result_data in game_data['results'].items():\n",
    "        new_id = text_to_id(result_data['question'])\n",
    "        new_results[new_id] = result_data\n",
    "    \n",
    "    # Replace the results dict\n",
    "    game_data['results'] = new_results\n",
    "    \n",
    "    with open(fname, 'w', encoding='utf-8') as f:\n",
    "        json.dump(game_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ec9026e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate IDs found in gemini-2.0-flash-001_phase1_compiled.json\n",
      "No duplicate IDs found in deepseek-chat_phase1_compiled.json\n",
      "No duplicate IDs found in gemini-2.5-flash-preview-04-17_phase1_compiled.json\n",
      "No duplicate IDs found in grok-3-latest_phase1_compiled.json\n",
      "No duplicate IDs found in gpt-4o-2024-08-06_phase1_compiled.json\n",
      "No duplicate IDs found in claude-sonnet-4-20250514_phase1_compiled.json\n",
      "No duplicate IDs found in claude-3-5-sonnet-20241022_phase1_compiled.json\n"
     ]
    }
   ],
   "source": [
    "## check for duplicate ids\n",
    "from collections import Counter\n",
    "targ_dir = \"compiled_results_sqa\"\n",
    "for filename in os.listdir(targ_dir):\n",
    "   if filename.endswith(\".json\"):\n",
    "    fname = os.path.join(targ_dir, filename)\n",
    "    with open(fname, 'r', encoding='utf-8') as f:\n",
    "        game_data = json.load(f)\n",
    "    id_counts = Counter(game_data['results'].keys())\n",
    "    duplicates = [id for id, count in id_counts.items() if count > 1]\n",
    "    if duplicates:\n",
    "        print(f\"Duplicate IDs found in {filename}: {duplicates}\")\n",
    "    else:\n",
    "        print(f\"No duplicate IDs found in {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55bb966f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 158 trials with delegation differences between history and no history versions.\n",
      "Found 19 trials with choice differences between history and no history versions.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "f1 = \"./delegate_game_logs/claude-3-5-sonnet-20241022_GPSA_50_450_team0.7_temp0.0_1749479584_game_data_evaluated.json\"\n",
    "f2 = \"./delegate_game_logs/claude-3-5-sonnet-20241022_GPSA_50_450_nohistory_summary_team0.5_temp0.0_1749559243_game_data_evaluated.json\"\n",
    "with open(f1, 'r', encoding='utf-8') as f:\n",
    "    game_data_hist = json.load(f)\n",
    "with open(f2, 'r', encoding='utf-8') as f:\n",
    "    game_data_nohist = json.load(f)\n",
    "\n",
    "diff_del_list = []\n",
    "diff_choice_list = []\n",
    "for trial in game_data_hist['results']:\n",
    "    qid = trial['question_id']\n",
    "    for trial_nohist in game_data_nohist['results']:\n",
    "        if trial_nohist['question_id'] == qid:\n",
    "            if trial['delegation_choice'] != trial_nohist['delegation_choice']:\n",
    "                diff_del_list.append((trial, trial_nohist))\n",
    "            elif (trial['delegation_choice']==\"Self\" and trial['subject_correct'] != trial_nohist['subject_correct']):\n",
    "                diff_choice_list.append((trial, trial_nohist))\n",
    "            break\n",
    "print(f\"Found {len(diff_del_list)} trials with delegation differences between history and no history versions.\")\n",
    "print(f\"Found {len(diff_choice_list)} trials with choice differences between history and no history versions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "526064ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'subject_id': 'claude-3-5-sonnet-20241022_GPSA_50_450_team0.7_temp0.0',\n",
       "  'phase': 2,\n",
       "  'trial_in_phase': 193,\n",
       "  'question_id': 'gpqa_train_recjgMJaMxz4ESDF2',\n",
       "  'question_text': \"Compounds that have the same molecular formula but are different in their structural arrangement are known as isomers. Isomers have two types, constitutional isomers and stereoisomers. Constitutional isomers have the same molecular formula but differ in their structures. In stereoisomers, molecules are connected in the same way, but their arrangements in space are different.\\nWhich of the following organic moieties show optical isomerism?\\n\\n1. dimethyl 6,6'-dinitro-[1,1'-biphenyl]-2,2'-dicarboxylate\\n2. methyl 2-hydroxypropanoate\\n3. benzophenone\\n4. dimethyl fumarate\",\n",
       "  'correct_answer': '1 and 2',\n",
       "  'timestamp': 1749480176.218984,\n",
       "  'subject_answer': '2 (methyl 2-hydroxypropanoate)\\n\\nThis compound has a chiral carbon with four different substituents: -H, -OH, -CH3, and -COOCH3.',\n",
       "  'subject_correct': False,\n",
       "  'teammate_answer': None,\n",
       "  'teammate_correct': None,\n",
       "  'delegation_choice': 'Self',\n",
       "  'team_answer': '2 (methyl 2-hydroxypropanoate)\\n\\nThis compound has a chiral carbon with four different substituents: -H, -OH, -CH3, and -COOCH3.',\n",
       "  'team_correct': False,\n",
       "  'probs': None,\n",
       "  'evaluation_method': 'llm_plurality',\n",
       "  'judgments': {'gpt-4o-2024-08-06': 'NO',\n",
       "   'gemini-2.0-flash-001': 'NO',\n",
       "   'deepseek-chat': 'NO'}},\n",
       " {'subject_id': 'claude-3-5-sonnet-20241022_GPSA_50_450_nohistory_summary_team0.5_temp0.0',\n",
       "  'phase': 2,\n",
       "  'trial_in_phase': 193,\n",
       "  'question_id': 'gpqa_train_recjgMJaMxz4ESDF2',\n",
       "  'question_text': \"Compounds that have the same molecular formula but are different in their structural arrangement are known as isomers. Isomers have two types, constitutional isomers and stereoisomers. Constitutional isomers have the same molecular formula but differ in their structures. In stereoisomers, molecules are connected in the same way, but their arrangements in space are different.\\nWhich of the following organic moieties show optical isomerism?\\n\\n1. dimethyl 6,6'-dinitro-[1,1'-biphenyl]-2,2'-dicarboxylate\\n2. methyl 2-hydroxypropanoate\\n3. benzophenone\\n4. dimethyl fumarate\",\n",
       "  'correct_answer': '1 and 2',\n",
       "  'timestamp': 1749560101.7081861,\n",
       "  'subject_answer': 'Let me analyze this carefully. To show optical isomerism, a molecule needs to have a chiral center (typically a carbon with 4 different substituents) or axis of chirality.\\n\\nLooking at the options:\\n1. Biphenyl derivative - can show axial chirality due to restricted rotation\\n2. Methyl 2-hydroxypropanoate - has a chiral center (carbon with -OH, -COOCH3, -CH3, and -H)\\n3. Benzophenone - no chiral center\\n4. Dimethyl fumarate - no chiral center\\n\\nOptions 1 and 2 show optical isomerism.\\n\\nThe answer is: 1, 2',\n",
       "  'subject_correct': True,\n",
       "  'teammate_answer': None,\n",
       "  'teammate_correct': None,\n",
       "  'delegation_choice': 'Self',\n",
       "  'team_answer': 'Let me analyze this carefully. To show optical isomerism, a molecule needs to have a chiral center (typically a carbon with 4 different substituents) or axis of chirality.\\n\\nLooking at the options:\\n1. Biphenyl derivative - can show axial chirality due to restricted rotation\\n2. Methyl 2-hydroxypropanoate - has a chiral center (carbon with -OH, -COOCH3, -CH3, and -H)\\n3. Benzophenone - no chiral center\\n4. Dimethyl fumarate - no chiral center\\n\\nOptions 1 and 2 show optical isomerism.\\n\\nThe answer is: 1, 2',\n",
       "  'team_correct': True,\n",
       "  'probs': None,\n",
       "  'evaluation_method': 'llm_plurality',\n",
       "  'judgments': {'gpt-4o-2024-08-06': 'YES',\n",
       "   'gemini-2.0-flash-001': 'YES',\n",
       "   'deepseek-chat': 'YES'}})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_choice_list[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a0fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contingency(delegate: np.ndarray, correct: np.ndarray):\n",
    "    \"\"\"\n",
    "    delegate : bool[N]   True -> model delegated\n",
    "    correct  : bool[N]   True -> model would be correct on its own\n",
    "    returns  : TP, FN, FP, TN as ints\n",
    "    \"\"\"\n",
    "    TP = np.sum(delegate  & ~correct)   # delegate & wrong\n",
    "    FN = np.sum(~delegate & ~correct)   # keep     & wrong\n",
    "    FP = np.sum(delegate  &  correct)   # delegate & right\n",
    "    TN = np.sum(~delegate &  correct)   # keep     & right\n",
    "    return TP, FN, FP, TN\n",
    "\n",
    "def lift_mcc_stats(tp, fn, fp, tn, p0, n_boot=2000, seed=0):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tp, fn, fp, tn : int\n",
    "        Contingency counts on Phase-2 items  \n",
    "            tp = delegate & wrong  \n",
    "            fn = keep & wrong  \n",
    "            fp = delegate & right  \n",
    "            tn = keep & right\n",
    "    p0 : float\n",
    "        Baseline accuracy to test against (global for RAW, hybrid value for HYBRID)\n",
    "    Returns\n",
    "    -------\n",
    "    dict with point estimates, CIs, and p-values for\n",
    "        lift   = acc_kept - p0\n",
    "        mcc    = Matthews correlation\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # ---------- point estimates --------------------------------------------\n",
    "    k         = fn + tn                       # kept items\n",
    "    kept_acc  = tn / k if k else np.nan\n",
    "    lift      = kept_acc - p0\n",
    "\n",
    "    denom = math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    mcc   = (tp*tn - fp*fn) / denom if denom else np.nan\n",
    "\n",
    "    # ---------- p-values ----------------------------------------------------\n",
    "    p_lift = binomtest(tn, k, p0, alternative='two-sided').pvalue\n",
    "    p_mcc  = mcnemar([[tn, fp],\n",
    "                      [fn, tp]], exact=True).pvalue   # two-sided by default\n",
    "\n",
    "    # ---------- bootstrap CIs ----------------------------------------------\n",
    "    N        = tp + fn + fp + tn\n",
    "    counts   = np.array([tp, fn, fp, tn], int)\n",
    "    probs    = counts / N\n",
    "\n",
    "    lifts, mccs = [], []\n",
    "    for _ in range(n_boot):\n",
    "        sample = rng.choice(4, size=N, replace=True, p=probs)\n",
    "        btp, bfn, bfp, btn = np.bincount(sample, minlength=4)\n",
    "\n",
    "        bk     = bfn + btn\n",
    "        b_acc  = btn / bk if bk else 0.0\n",
    "        lifts.append(b_acc - p0)\n",
    "\n",
    "        bden = math.sqrt((btp+bfp)*(btp+bfn)*(btn+bfp)*(btn+bfn))\n",
    "        bmcc = (btp*btn - bfp*bfn) / bden if bden else 0.0\n",
    "        mccs.append(bmcc)\n",
    "\n",
    "    ci_lift = np.percentile(lifts, [2.5, 97.5])\n",
    "    ci_mcc  = np.percentile(mccs,  [2.5, 97.5])\n",
    "\n",
    "    return dict(\n",
    "        lift       = lift,\n",
    "        lift_ci    = tuple(ci_lift),\n",
    "        p_lift     = p_lift,\n",
    "        mcc        = mcc,\n",
    "        mcc_ci     = tuple(ci_mcc),\n",
    "        p_mcc      = p_mcc\n",
    "    )\n",
    "\n",
    "...\n",
    "\n",
    "delegated = np.array(df_model['delegate_choice'], bool)\n",
    "kept_mask = ~delegated                       # True where model answered itself\n",
    "cap_corr = np.array(df_model['s_i_capability'], int)   # Baseline correctness from capabilities file\n",
    "team_corr = np.where(df_model['delegate_choice'] == 0, df_model['team_correct'].fillna(0).astype(int), 0).astype(int) #Real in-game self correctness (only defined when kept)\n",
    "# Hybrid correctness label \n",
    "#    – use real game correctness when the model kept\n",
    "#    – fallback to baseline correctness when it delegated\n",
    "true_label = np.where(kept_mask, team_corr, cap_corr)   # 1 = model would be correct\n",
    "\n",
    "TP, FN, FP, TN = contingency(delegated, cap_corr)\n",
    "raw_stats = lift_mcc_stats(TP, FN, FP, TN, cap_corr.mean())\n",
    "log_output(f\"Introspection score = {raw_stats['mcc']:.3f} [{raw_stats['mcc_ci'][0]:.3f}, {raw_stats['mcc_ci'][1]:.3f}], p={raw_stats['p_mcc']:.4g}\")\n",
    "delta_d, ci_low, ci_high, p_val = delegate_gap_stats(TP=TP, FN=FN, FP=FP, TN=TN)\n",
    "log_output(f\"Delegate Gap = {delta_d:.3f} [{ci_low:.3f}, {ci_high:.3f}, p={p_val:.4g}]\")\n",
    "\n",
    "TP, FN, FP, TN = contingency(delegated, true_label)\n",
    "N = (TP+FP+TN+FN)\n",
    "k   = FN + TN\n",
    "acc_kept   = TN / k\n",
    "acc_deleg  = cap_corr[delegated].mean()\n",
    "p0_hyb     = (k/N)*acc_kept + (1-k/N)*acc_deleg\n",
    "adj_stats = lift_mcc_stats(TP, FN, FP, TN, p0_hyb)\n",
    "\n",
    "log_output(f\"Adjusted introspection score = {adj_stats['mcc']:.3f} [{adj_stats['mcc_ci'][0]:.3f}, {adj_stats['mcc_ci'][1]:.3f}], p={adj_stats['p_mcc']:.4g}\")\n",
    "delta_d, ci_low, ci_high, p_val = delegate_gap_stats(TP=TP, FN=FN, FP=FP, TN=TN)\n",
    "log_output(f\"Adjusted delegate gap = {delta_d:.3f} [{ci_low:.3f}, {ci_high:.3f}, p={p_val:.4g}]\")\n",
    "\n",
    "log_output(f\"Self-acc lift = {raw_stats['lift']:.3f} [{raw_stats['lift_ci'][0]:.3f}, {raw_stats['lift_ci'][1]:.3f}], p={raw_stats['p_lift']:.4g}\")\n",
    "\n",
    "log_output(f\"Adjusted self-acc lift = {adj_stats['lift']:.3f} [{adj_stats['lift_ci'][0]:.3f}, {adj_stats['lift_ci'][1]:.3f}], p={adj_stats['p_lift']:.4g}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
