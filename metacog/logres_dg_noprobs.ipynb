{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab14e8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading main GPQA dataset for features...\n",
      "Attempting to load GPQA (train split)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since Idavidrein/gpqa couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'gpqa_main' at /Users/christopherackerman/.cache/huggingface/datasets/Idavidrein___gpqa/gpqa_main/0.0.0/90b8e5be2b1d3d2dbfe016cdab47981150600c4a (last modified on Tue May 20 13:25:45 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPQA Dataset loaded successfully.\n",
      "Formatting 448 questions from GPQA...\n",
      "Warning: Only able to format 447 unique questions, but 448 were requested.\n",
      "Successfully formatted 447 unique questions from GPQA.\n",
      "GPQA feature lookup created with 447 entries.\n",
      "\n",
      "--- Analyzing Model from Game File: claude-3-5-sonnet-20241022_GPQA_100_100_team0.6_1747406864_game_data.json (feedback=False) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: claude-3-5-sonnet-20241022_GPQA_100_100_team0.6_1747407886_game_data.json (feedback=True) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: claude-3-5-sonnet-20241022_GPQA_100_100_team0.6_temp0.0_nobio_1747770061_game_data.json (feedback=False) ---\n",
      "\n",
      "  Skipping Model 4 (full controls) due to insufficient domain variance or data points.\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: claude-3-5-sonnet-20241022_GPQA_50_100_team0.65_1747405864_game_data.json (feedback=False) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: claude-3-5-sonnet-20241022_GPQA_50_100_team0.6_1747406304_game_data.json (feedback=False) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: claude-3-5-sonnet-20241022_GPQA_50_100_team0.7_1747405328_game_data.json (feedback=False) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: claude-3-7-sonnet-20250219_GPQA_100_100_team0.65_1747409495_game_data.json (feedback=False) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: claude-3-7-sonnet-20250219_GPQA_100_100_team0.65_1747411273_game_data.json (feedback=True) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: claude-3-haiku-20240307_GPQA_50_100_team0.7_1747595591_game_data.json (feedback=False) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: claude-3-opus-20240229_GPQA_100_100_team0.55_1747415110_game_data.json (feedback=True) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: claude-3-opus-20240229_GPQA_100_100_team0.6_1747413521_game_data.json (feedback=False) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: claude-3-opus-20240229_GPQA_100_100_team0.6_1747414339_game_data.json (feedback=True) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: claude-3-sonnet-20240229_GPQA_50_100_team0.7_1747596226_game_data.json (feedback=False) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: gemini-1.5-pro_GPQA_50_200_team0.6_1747431236_game_data.json (feedback=False) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: gemini-1.5-pro_GPQA_50_200_team0.7_1747431651_game_data.json (feedback=False) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: gemini-1.5-pro_GPQA_50_200_team0.7_1747432049_game_data.json (feedback=True) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: gemini-2.0-flash-001_GPQA_50_200_team0.5_temp0.0_nobio_1747781783_game_data.json (feedback=False) ---\n",
      "\n",
      "  Skipping Model 4 (full controls) due to insufficient domain variance or data points.\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: gemini-2.0-flash-001_GPQA_50_200_team0.5_temp0.0_nobio_noeasy_1747780809_game_data.json (feedback=False) ---\n",
      "\n",
      "  Skipping Model 4 (full controls) due to insufficient domain variance or data points.\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: gemini-2.0-flash-001_GPQA_50_200_team0.5_temp0.0_noctr_1747791168_game_data.json (feedback=False) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: gemini-2.0-flash-001_GPQA_50_200_team0.5_temp0.0_noctr_nobio_1747790761_game_data.json (feedback=False) ---\n",
      "\n",
      "  Skipping Model 4 (full controls) due to insufficient domain variance or data points.\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: gemini-2.0-flash-001_GPQA_50_200_team0.5_temp0.0_noctr_nobio_noeasy_1747790955_game_data.json (feedback=False) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherackerman/repos/self_awareness/venv312/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Skipping Model 4 (full controls) due to insufficient domain variance or data points.\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: gemini-2.0-flash-001_GPQA_50_200_team0.5_temp0.0_noctr_noeasy_1747791410_game_data.json (feedback=False) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: gemini-2.0-flash-001_GPQA_50_300_team0.6_1747420101_game_data.json (feedback=False) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: gemini-2.0-flash-001_GPQA_50_300_team0.6_1747420378_game_data.json (feedback=True) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: gemini-2.0-flash-001_GPQA_50_300_team0.6_temp0.0_nobio_1747769347_game_data.json (feedback=False) ---\n",
      "\n",
      "  Skipping Model 4 (full controls) due to insufficient domain variance or data points.\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: gpt-4-turbo-2024-04-09_GPQA_50_200_team0.6_1747424007_game_data.json (feedback=False) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: gpt-4-turbo-2024-04-09_GPQA_50_200_team0.6_1747424300_game_data.json (feedback=True) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: grok-3-latest_GPQA_50_200_team0.75_1747442192_game_data.json (feedback=False) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: grok-3-latest_GPQA_50_200_team0.7_1747441815_game_data.json (feedback=False) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherackerman/repos/self_awareness/venv312/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/christopherackerman/repos/self_awareness/venv312/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/christopherackerman/repos/self_awareness/venv312/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/christopherackerman/repos/self_awareness/venv312/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: grok-3-latest_GPQA_50_200_team0.8_1747443654_game_data.json (feedback=False) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: grok-3-latest_GPQA_50_200_team0.8_1747449213_game_data.json (feedback=True) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: grok-3-latest_GPQA_50_250_team0.8_1747448729_game_data.json (feedback=False) ---\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Model from Game File: meta-llama-Meta-Llama-3.1-405B-Instruct_GPQA_50_100_team0.7_1747490845_game_data.json (feedback=True) ---\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherackerman/repos/self_awareness/venv312/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/var/folders/rz/_cb3hkpx005f_j2_81th4pqr0000gn/T/ipykernel_96554/3814465589.py:240: RuntimeWarning: overflow encountered in exp\n",
      "  ci_upper_or = np.exp(-conf_int_s_i_log_odds.iloc[0]) # Exponentiate the negative of the lower bound of original coef CI\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from load_and_format_datasets import load_and_format_dataset\n",
    "import re\n",
    "\n",
    "LOG_FILENAME = \"analysis_log_logres_dg_gpqa.txt\"\n",
    "\n",
    "def log_output(message_string, print_to_console=False):\n",
    "    with open(LOG_FILENAME, 'a', encoding='utf-8') as f:\n",
    "        f.write(str(message_string) + \"\\n\")\n",
    "    if print_to_console:\n",
    "        print(message_string)\n",
    "\n",
    "LOG_METRICS_TO_EXTRACT = [\n",
    "    \"Delegation to teammate occurred\",\n",
    "    \"Phase 1 self-accuracy (from completed results, total - phase2)\",\n",
    "    \"Phase 2 self-accuracy\",\n",
    "    \"Statistical test (P2 self vs P1)\"\n",
    "]\n",
    "\n",
    "LOG_METRIC_PATTERNS = {\n",
    "    \"Delegation to teammate occurred\": re.compile(r\"^\\s*Delegation to teammate occurred in (.*)$\"),\n",
    "    \"Phase 1 self-accuracy (from completed results, total - phase2)\": re.compile(r\"^\\s*Phase 1 self-accuracy \\(from completed results, total - phase2\\): (.*)$\"),\n",
    "    \"Phase 2 self-accuracy\": re.compile(r\"^\\s*Phase 2 self-accuracy: (.*)$\"),\n",
    "    \"Statistical test (P2 self vs P1)\": re.compile(r\"^\\s*Statistical test \\(P2 self vs P1\\): (.*)$\")\n",
    "}\n",
    "\n",
    "def extract_log_file_metrics(log_filepath):\n",
    "    \"\"\"Reads a .log file and extracts specified metrics.\"\"\"\n",
    "    extracted_log_metrics = {key: \"Not found\" for key in LOG_METRICS_TO_EXTRACT}\n",
    "    try:\n",
    "        with open(log_filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                for metric_name, pattern in LOG_METRIC_PATTERNS.items():\n",
    "                    match = pattern.match(line)\n",
    "                    if match:\n",
    "                        extracted_log_metrics[metric_name] = match.group(1).strip()\n",
    "                        # Optimization: if all log metrics found, can break early\n",
    "                        # This requires checking if all \"Not found\" have been replaced\n",
    "                        if all(val != \"Not found\" for val in extracted_log_metrics.values()):\n",
    "                            return extracted_log_metrics\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Log file not found: {log_filepath}\")\n",
    "        # Return dict with \"Not found\" for all log metrics\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading log file {log_filepath}: {e}\")\n",
    "        # Return dict with \"Not found\" for all log metrics\n",
    "    return extracted_log_metrics\n",
    "\n",
    "def get_average_word_length(question_text):\n",
    "    \"\"\"Calculates the average word length in the question.\"\"\"\n",
    "    if not isinstance(question_text, str):\n",
    "        return 0\n",
    "    words = re.findall(r'\\b\\w+\\b', question_text.lower()) # Find all words\n",
    "    if not words:\n",
    "        return 0\n",
    "    total_word_length = sum(len(word) for word in words)\n",
    "    return total_word_length / len(words)\n",
    "\n",
    "def get_percent_non_alphabetic_whitespace(question_text):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of characters in the question text that are\n",
    "    not alphabetic, not numeric, and not whitespace.\n",
    "    \"\"\"\n",
    "    if not isinstance(question_text, str) or len(question_text) == 0:\n",
    "        return 0\n",
    "    \n",
    "    non_alphabetic_whitespace_chars = re.findall(r'[^a-zA-Z\\s]', question_text)\n",
    "    return (len(non_alphabetic_whitespace_chars) / len(question_text)) * 100\n",
    "\n",
    "def prepare_regression_data_for_model(game_file_path, \n",
    "                                      gpqa_feature_lookup, \n",
    "                                      capabilities_s_i_map_for_model):\n",
    "    \"\"\"\n",
    "    Prepares a DataFrame for a single model's game file.\n",
    "    \n",
    "    Args:\n",
    "        game_file_path (str): Path to the _game_data.json file.\n",
    "        gpqa_feature_lookup (dict): Maps q_id to {'difficulty': score, 'domain': str, 'q_text': str}.\n",
    "        capabilities_s_i_map_for_model (dict): Maps q_id to S_i (0 or 1) for THIS model.\n",
    "                                            This map should be from the model's specific \n",
    "                                            _phase1_completed.json (capabilities) file.\n",
    "    Returns:\n",
    "        pandas.DataFrame or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(game_file_path, 'r', encoding='utf-8') as f:\n",
    "            game_data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading game file {game_file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    phase1_subject_feedback = game_data[\"feedback_config\"][\"phase1_subject_feedback\"]\n",
    "\n",
    "    phase2_trials = [t for t in game_data.get(\"results\", []) if t.get('phase') == 2]\n",
    "    if not phase2_trials:\n",
    "        return None\n",
    "\n",
    "    regression_data = []\n",
    "    for trial in phase2_trials:\n",
    "        q_id = trial.get(\"question_id\")\n",
    "        delegation_choice_str = trial.get(\"delegation_choice\")\n",
    "\n",
    "        if not q_id or not delegation_choice_str:\n",
    "            continue\n",
    "\n",
    "        gpqa_features = gpqa_feature_lookup.get(q_id)\n",
    "        s_i_capability = capabilities_s_i_map_for_model.get(q_id) # S_i specific to this model\n",
    "        domain = gpqa_features.get('domain', 'unknown').replace(' ', '_').lower()\n",
    "\n",
    "        if gpqa_features and gpqa_features.get('difficulty') is not None and s_i_capability is not None:\n",
    "            delegate_choice_numeric = 1 if delegation_choice_str == \"Teammate\" else 0\n",
    "            regression_data.append({\n",
    "                'delegate_choice': delegate_choice_numeric,\n",
    "                's_i_capability': s_i_capability,\n",
    "                'human_difficulty': gpqa_features['difficulty'],\n",
    "                'q_length': len(gpqa_features.get('q_text', '')),\n",
    "                'domain': (\"Biology\" if domain == \"biology\" else \"NonBiology\"),\n",
    "                'overlap_ratio': gpqa_features.get('overlap_ratio', 0),\n",
    "                'avg_word_length': get_average_word_length(gpqa_features.get('q_text', '')),\n",
    "                'percent_non_alphabetic_whitespace': get_percent_non_alphabetic_whitespace(gpqa_features.get('q_text', '')),\n",
    "                # Add other surface features here if you want\n",
    "            })\n",
    "    \n",
    "    if not regression_data:\n",
    "        return None, None\n",
    "    return pd.DataFrame(regression_data), phase1_subject_feedback\n",
    "\n",
    "# --- Main Analysis Logic ---\n",
    "\n",
    "# 1. Load GPQA data once for features (difficulty, domain, question text for length)\n",
    "print(\"Loading main GPQA dataset for features...\")\n",
    "gpqa_all_questions = load_and_format_dataset(\"GPQA\") # This should have id, Question, high_level_domain, difficulty_score\n",
    "\n",
    "gpqa_feature_lookup = {\n",
    "    item['id']: {\n",
    "        'overlap_ratio': item.get('overlap_ratio', 0),\n",
    "        'difficulty': item['difficulty_score'],\n",
    "        'domain': item['high_level_domain'],\n",
    "        'q_text': item['question']\n",
    "    } for item in gpqa_all_questions\n",
    "}\n",
    "print(f\"GPQA feature lookup created with {len(gpqa_feature_lookup)} entries.\")\n",
    "\n",
    "\n",
    "# 2. Specify directories\n",
    "game_logs_dir = \"./delegate_game_logs/\"       # Where your _game_data.json files are\n",
    "capabilities_dir = \"./completed_results_gpqa/\" # Where your _phase1_completed.json files are\n",
    "\n",
    "if not os.path.isdir(game_logs_dir) or not os.path.isdir(capabilities_dir):\n",
    "    print(f\"Error: Ensure directories exist: {game_logs_dir}, {capabilities_dir}\")\n",
    "    exit()\n",
    "\n",
    "# 3. Iterate through game log files\n",
    "for game_filename in sorted(os.listdir(game_logs_dir)):\n",
    "    if game_filename.endswith(\"_game_data.json\") and \"_GPQA_\" in game_filename:\n",
    "        \n",
    "        # Derive capabilities filename (assuming a consistent naming pattern)\n",
    "        # E.g., \"modelname_GPQA_params_game_data.json\" -> \"modelname_phase1_completed.json\"\n",
    "        # This needs to match your actual naming convention.\n",
    "        # Example: if game_filename is \"claude-3-opus..._GPQA_100_100_team0.6_12345_game_data.json\"\n",
    "        # We need to extract \"claude-3-opus...\" part.\n",
    "        model_name_part = game_filename.split(\"_GPQA_\")[0]\n",
    "        capabilities_filename = f\"{model_name_part}_phase1_completed.json\"\n",
    "        capabilities_file_path = os.path.join(capabilities_dir, capabilities_filename)\n",
    "\n",
    "        if not os.path.exists(capabilities_file_path):\n",
    "            print(f\"  Corresponding capabilities file not found: {capabilities_file_path}. Skipping model.\")\n",
    "            continue\n",
    "\n",
    "        # Load S_i data for this specific model from its capabilities file\n",
    "        s_i_map_for_this_model = {}\n",
    "        try:\n",
    "            with open(capabilities_file_path, 'r', encoding='utf-8') as f_cap:\n",
    "                cap_data = json.load(f_cap)\n",
    "            for q_id, res_info in cap_data.get(\"results\", {}).items():\n",
    "                if res_info.get(\"is_correct\") is not None:\n",
    "                    s_i_map_for_this_model[q_id] = 1 if res_info[\"is_correct\"] else 0\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading capabilities file {capabilities_file_path}: {e}. Skipping model.\")\n",
    "            continue\n",
    "        \n",
    "        if not s_i_map_for_this_model:\n",
    "            print(f\"  No S_i data loaded from {capabilities_file_path}. Skipping model.\")\n",
    "            continue\n",
    "\n",
    "        # Prepare data for this model's game\n",
    "        game_file_path = os.path.join(game_logs_dir, game_filename)\n",
    "        df_model, phase1_subject_feedback = prepare_regression_data_for_model(game_file_path, \n",
    "                                                     gpqa_feature_lookup, \n",
    "                                                     s_i_map_for_this_model)\n",
    "\n",
    "        if df_model is None or df_model.empty:\n",
    "            print(\"  No data for regression analysis for this file.\")\n",
    "            continue\n",
    "        \n",
    "        log_output(f\"\\n--- Analyzing Model from Game File: {game_filename} (feedback={phase1_subject_feedback}) ---\", print_to_console=True)\n",
    "        log_metrics_dict = extract_log_file_metrics(game_file_path.replace(\"_game_data.json\", \".log\"))\n",
    "        for metric, value in log_metrics_dict.items():\n",
    "            log_output(f\"  {metric}: {value}\")\n",
    "\n",
    "        # Run Logistic Regressions\n",
    "        try:\n",
    "            log_output(\"\\n  Model 1: Delegate_Choice ~ S_i_capability\")\n",
    "            logit_model1 = smf.logit('delegate_choice ~ s_i_capability', data=df_model).fit(disp=0)\n",
    "            log_output(logit_model1.summary())\n",
    "\n",
    "            log_output(\"\\n  Model 2: Delegate_Choice ~ human_difficulty\")\n",
    "            logit_model2 = smf.logit('delegate_choice ~ human_difficulty', data=df_model).fit(disp=0)\n",
    "            log_output(logit_model2.summary())\n",
    "\n",
    "            log_output(\"\\n  Model 3: Delegate_Choice ~ S_i_capability + human_difficulty\")\n",
    "            logit_model3 = smf.logit('delegate_choice ~ s_i_capability + human_difficulty', data=df_model).fit(disp=0)\n",
    "            log_output(logit_model3.summary())\n",
    "            \n",
    "            # Optional: Full model with controls like q_length and domain\n",
    "            # Ensure domain has enough categories and data points\n",
    "            if df_model['domain'].nunique() > 1 and len(df_model) > 20 : # Heuristic checks\n",
    "                 model_def_str = 'delegate_choice ~ s_i_capability + human_difficulty + q_length + C(domain) + overlap_ratio + avg_word_length + percent_non_alphabetic_whitespace'\n",
    "                 log_output(f\"\\n  Model 4: {model_def_str.capitalize()}\")\n",
    "                 try:\n",
    "                    logit_model4 = smf.logit(model_def_str, data=df_model).fit(disp=0)\n",
    "                    log_output(logit_model4.summary())\n",
    "                    coef_s_i = logit_model4.params.get('s_i_capability')\n",
    "                    pval_s_i = logit_model4.pvalues.get('s_i_capability')\n",
    "                    conf_int_s_i_log_odds = logit_model4.conf_int().loc['s_i_capability']\n",
    "                    odds_ratio_delegate_Si0_vs_Si1 = np.exp(-coef_s_i)\n",
    "                    ci_lower_or = np.exp(-conf_int_s_i_log_odds.iloc[1]) # Exponentiate the negative of the upper bound of original coef CI\n",
    "                    ci_upper_or = np.exp(-conf_int_s_i_log_odds.iloc[0]) # Exponentiate the negative of the lower bound of original coef CI    \n",
    "                    log_output(f\"\\n--- Odds Ratio for S_i_capability on Delegation (Adjusted) ---\")\n",
    "                    log_output(f\"P-value for s_i_capability: {pval_s_i:.4g}\")\n",
    "                    log_output(f\"Odds Ratio (Delegating when S_i=0 vs. S_i=1): {odds_ratio_delegate_Si0_vs_Si1:.4f}\")\n",
    "                    log_output(f\"95% CI for this Odds Ratio: [{ci_lower_or:.4f}, {ci_upper_or:.4f}]\")\n",
    "                 except Exception as e_full:\n",
    "                     log_output(f\"    Could not fit full model: {e_full}\") # E.g. perfect separation from domain\n",
    "            else:\n",
    "                 log_output(\"\\n  Skipping Model 4 (full controls) due to insufficient domain variance or data points.\", print_to_console=True)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error during logistic regression for {game_filename}: {e}\")\n",
    "        \n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381143fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from load_and_format_datasets import load_and_format_dataset\n",
    "import re\n",
    "import pymc as pm\n",
    "from patsy import dmatrices\n",
    "from io import StringIO\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_introspection_vs_heuristics(df_model, depvar, keyvar, potential_heuristics):\n",
    "    \"\"\"\n",
    "    Compare capability-based, heuristic-based, and combined models\n",
    "    \n",
    "    Args:\n",
    "        df_model: DataFrame with 'delegate_choice', 's_i_capability', and heuristic columns\n",
    "        potential_heuristics: List of column names to consider as heuristics\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with AIC values and comparisons\n",
    "    \"\"\"\n",
    "    y = df_model[depvar]\n",
    "    \n",
    "    # Helper function to calculate AIC\n",
    "    def calculate_aic(y_true, y_pred_proba, n_params):\n",
    "        ll = -len(y_true) * log_loss(y_true, y_pred_proba)\n",
    "        aic = -2 * ll + 2 * n_params\n",
    "        return aic\n",
    "    \n",
    "    # 1. Capability-only model\n",
    "    X_cap = df_model[[keyvar]]\n",
    "    lr_capability = LogisticRegression(penalty=None)\n",
    "    lr_capability.fit(X_cap, y)\n",
    "    aic_capability = calculate_aic(y, lr_capability.predict_proba(X_cap)[:, 1], \n",
    "                                  X_cap.shape[1] + 1)  # +1 for intercept\n",
    "    \n",
    "    # 2. Heuristics model with LASSO selection\n",
    "    X_all_heuristics = pd.get_dummies(df_model[potential_heuristics])\n",
    "    #lasso = LogisticRegressionCV(penalty='l1', solver='liblinear', cv=5, random_state=42)\n",
    "    lasso = LogisticRegressionCV(penalty='l1', solver='liblinear', cv=5, random_state=42, Cs=np.logspace(-2, 1, 10))\n",
    "    lasso.fit(X_all_heuristics, y)\n",
    "    \n",
    "    # Get selected features (non-zero coefficients)\n",
    "    selected_features = X_all_heuristics.columns[lasso.coef_[0] != 0].tolist()\n",
    "\n",
    "\n",
    "    # Check if answer_type was properly one-hot encoded\n",
    "    print(f\"potential_heuristics: {potential_heuristics}\")\n",
    "    print(\"Columns created from answer_type_grouped:\")\n",
    "    print([col for col in X_all_heuristics.columns if 'answer_type' in col])\n",
    "\n",
    "    print(\"\\nDelegation rates by answer_type:\")\n",
    "    print(df_model.groupby('answer_type_grouped')['delegate_choice'].agg(['mean', 'count']))\n",
    "\n",
    "    # Correct attribute for LogisticRegressionCV\n",
    "    print(\"\\nLASSO regularization C:\", lasso.C_) \n",
    "    print(\"\\nAll LASSO coefficients:\")\n",
    "    for feat, coef in zip(X_all_heuristics.columns, lasso.coef_[0]):\n",
    "        if coef != 0:  # Only show non-zero coefficients\n",
    "            print(f\"{feat}: {coef:.4f}\")\n",
    "\n",
    "    if selected_features:\n",
    "        X_heur = X_all_heuristics[selected_features]\n",
    "        lr_heuristics = LogisticRegression(penalty=None)\n",
    "        lr_heuristics.fit(X_heur, y)\n",
    "        aic_heuristics = calculate_aic(y, lr_heuristics.predict_proba(X_heur)[:, 1], \n",
    "                                      X_heur.shape[1] + 1)\n",
    "    else:\n",
    "        # No heuristics selected - use intercept only\n",
    "        aic_heuristics = calculate_aic(y, [y.mean()]*len(y), 1)\n",
    "        selected_features = []\n",
    "    \n",
    "    # 3. Combined model\n",
    "    if selected_features:\n",
    "        X_combined = pd.concat([df_model[[keyvar]], X_heur], axis=1)\n",
    "    else:\n",
    "        X_combined = df_model[[keyvar]]\n",
    "    \n",
    "    lr_combined = LogisticRegression(penalty=None)\n",
    "    lr_combined.fit(X_combined, y)\n",
    "    aic_combined = calculate_aic(y, lr_combined.predict_proba(X_combined)[:, 1], \n",
    "                                X_combined.shape[1] + 1)\n",
    "    \n",
    "    # Calculate comparisons\n",
    "    delta_heur_vs_cap = aic_capability - aic_heuristics  # Positive = heuristics better\n",
    "    delta_comb_vs_heur = aic_heuristics - aic_combined   # Positive = combined better\n",
    "    delta_comb_vs_cap = aic_capability - aic_combined    # Positive = combined better\n",
    "    \n",
    "    # Interpret differences\n",
    "    def interpret_delta_aic(delta):\n",
    "        if abs(delta) < 2:\n",
    "            return \"no meaningful difference\"\n",
    "        elif abs(delta) < 4:\n",
    "            return \"weak evidence\"\n",
    "        elif abs(delta) < 7:\n",
    "            return \"moderate evidence\"\n",
    "        elif abs(delta) < 10:\n",
    "            return \"strong evidence\"\n",
    "        else:\n",
    "            return \"very strong evidence\"\n",
    "    \n",
    "    retstr = \"\"\n",
    "    retstr+=f\"Selected heuristics: {selected_features if selected_features else 'None'}\\n\"\n",
    "    retstr+=f\"\\nAIC values:\\n\"\n",
    "    retstr+=f\"  Capability only: {aic_capability:.1f}\\n\"\n",
    "    retstr+=f\"  Heuristics only: {aic_heuristics:.1f}\\n\"\n",
    "    retstr+=f\"  Combined: {aic_combined:.1f}\\n\"\n",
    "    \n",
    "    retstr+=f\"\\nModel comparisons:\\n\"\n",
    "    retstr+=f\"  Heuristics vs Capability: ΔAIC = {delta_heur_vs_cap:.1f}\\n\"\n",
    "    retstr+=f\"    → {interpret_delta_aic(delta_heur_vs_cap)} that heuristics are {'better' if delta_heur_vs_cap > 0 else 'worse'}\\n\"\n",
    "    \n",
    "    retstr+=f\"  Combined vs Heuristics: ΔAIC = {delta_comb_vs_heur:.1f}\\n\"\n",
    "    retstr+=f\"    → {interpret_delta_aic(delta_comb_vs_heur)} that capability adds value\\n\"\n",
    "    \n",
    "    retstr+=f\"  Combined vs Capability: ΔAIC = {delta_comb_vs_cap:.1f}\\n\"\n",
    "    retstr+=f\"    → {interpret_delta_aic(delta_comb_vs_cap)} that combined is {'better' if delta_comb_vs_cap > 0 else 'worse'}\\n\"\n",
    "    \n",
    "    return {\n",
    "        'aic_capability': aic_capability,\n",
    "        'aic_heuristics': aic_heuristics,\n",
    "        'aic_combined': aic_combined,\n",
    "        'delta_heur_vs_cap': delta_heur_vs_cap,\n",
    "        'delta_comb_vs_heur': delta_comb_vs_heur,\n",
    "        'delta_comb_vs_cap': delta_comb_vs_cap,\n",
    "        'selected_heuristics': selected_features\n",
    "    }, retstr\n",
    "\n",
    "\n",
    "class BayesianLogitResults:\n",
    "    def __init__(self, formula, data, trace, X_columns, n_obs):\n",
    "        self.formula = formula\n",
    "        self.n_obs = n_obs\n",
    "        self.trace = trace\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        posterior = trace.posterior.beta.values.reshape(-1, len(X_columns))\n",
    "        \n",
    "        self.params = pd.Series([np.mean(posterior[:, i]) for i in range(len(X_columns))], \n",
    "                               index=X_columns)\n",
    "        self.bse = pd.Series([np.std(posterior[:, i]) for i in range(len(X_columns))], \n",
    "                            index=X_columns)\n",
    "        self.tvalues = self.params / self.bse\n",
    "        \n",
    "        # P-value equivalents and confidence intervals\n",
    "        pvalues = []\n",
    "        conf_int_data = []\n",
    "        for i in range(len(X_columns)):\n",
    "            samples = posterior[:, i]\n",
    "            # CI\n",
    "            ci_low, ci_high = np.percentile(samples, [2.5, 97.5])\n",
    "            conf_int_data.append([ci_low, ci_high])\n",
    "            # P-value equivalent\n",
    "            if self.params.iloc[i] > 0:\n",
    "                p = np.mean(samples <= 0) * 2\n",
    "            else:\n",
    "                p = np.mean(samples >= 0) * 2\n",
    "            pvalues.append(min(p, 1.0))\n",
    "        \n",
    "        self.pvalues = pd.Series(pvalues, index=X_columns)\n",
    "        self.conf_int_df = pd.DataFrame(conf_int_data, index=X_columns, \n",
    "                                       columns=['[0.025', '0.975]'])\n",
    "    \n",
    "    def summary(self):\n",
    "        # Create summary DataFrame with exact statsmodels column names\n",
    "        summary_df = pd.DataFrame({\n",
    "            'coef': self.params,\n",
    "            'std err': self.bse,\n",
    "            'z': self.tvalues,\n",
    "            'P>|z|': self.pvalues,\n",
    "            '[0.025': self.conf_int_df['[0.025'],\n",
    "            '0.975]': self.conf_int_df['0.975]']\n",
    "        })\n",
    "        \n",
    "        # Create a custom object that mimics statsmodels Summary\n",
    "        class BayesianSummary:\n",
    "            def __init__(self, df, formula, n_obs):\n",
    "                self.df = df\n",
    "                self.formula = formula\n",
    "                self.n_obs = n_obs\n",
    "                \n",
    "            def __str__(self):\n",
    "                # Format exactly like statsmodels\n",
    "                buffer = StringIO()\n",
    "                buffer.write(\"                        Bayesian Logit Results                          \\n\")\n",
    "                buffer.write(\"========================================================================\\n\")\n",
    "                buffer.write(f\"Dep. Variable:        {self.formula.split('~')[0].strip():<20} No. Observations:    {self.n_obs:>6}\\n\")\n",
    "                buffer.write(\"Model:                Bayesian Logit    Method:              MCMC (PyMC)\\n\")\n",
    "                buffer.write(\"========================================================================\\n\")\n",
    "                \n",
    "                # Use pandas string formatting with proper width control\n",
    "                with pd.option_context('display.max_columns', None, \n",
    "                                     'display.width', 120,\n",
    "                                     'display.float_format', '{:>10.4f}'.format):\n",
    "                    buffer.write(self.df.to_string())\n",
    "                \n",
    "                buffer.write(\"\\n========================================================================\\n\")\n",
    "                buffer.write(\"Note: 'P>|z|' represents Bayesian equivalent of p-value\")\n",
    "                return buffer.getvalue()\n",
    "            \n",
    "            def __repr__(self):\n",
    "                return self.__str__()\n",
    "        \n",
    "        return BayesianSummary(summary_df, self.formula, self.n_obs)\n",
    "\n",
    "def bayesian_logit(formula, data, random_seed=42):\n",
    "    # Create design matrix\n",
    "    y, X = dmatrices(formula, data, return_type='dataframe')\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        # Weakly informative priors\n",
    "        beta = pm.Normal('beta', mu=0, sigma=2.5, shape=X.shape[1])\n",
    "        \n",
    "        # Linear combination\n",
    "        eta = pm.math.dot(X, beta)\n",
    "        \n",
    "        # Likelihood\n",
    "        y_obs = pm.Bernoulli('y_obs', p=pm.math.sigmoid(eta), observed=y.values.ravel())\n",
    "        \n",
    "        # Sample\n",
    "        trace = pm.sample(2000, tune=1000, chains=2, cores=1, \n",
    "                         progressbar=False, return_inferencedata=True, random_seed=random_seed)\n",
    "    \n",
    "    return BayesianLogitResults(formula, data, trace, X.columns, len(data))\n",
    "\n",
    "LOG_FILENAME = \"analysis_log_logres_dg_sqa.txt\"\n",
    "\n",
    "def log_output(message_string, print_to_console=False):\n",
    "    with open(LOG_FILENAME, 'a', encoding='utf-8') as f:\n",
    "        f.write(str(message_string) + \"\\n\")\n",
    "    if print_to_console:\n",
    "        print(message_string)\n",
    "\n",
    "LOG_METRICS_TO_EXTRACT = [\n",
    "    \"Delegation to teammate occurred\",\n",
    "    \"Phase 1 self-accuracy (from completed results, total - phase2)\",\n",
    "    \"Phase 2 self-accuracy\",\n",
    "    \"Statistical test (P2 self vs P1)\"\n",
    "]\n",
    "\n",
    "LOG_METRIC_PATTERNS = {\n",
    "    \"Delegation to teammate occurred\": re.compile(r\"^\\s*Delegation to teammate occurred in (.*)$\"),\n",
    "    \"Phase 1 self-accuracy (from completed results, total - phase2)\": re.compile(r\"^\\s*Phase 1 self-accuracy \\(from completed results, total - phase2\\): (.*)$\"),\n",
    "    \"Phase 2 self-accuracy\": re.compile(r\"^\\s*Phase 2 self-accuracy: (.*)$\"),\n",
    "    \"Statistical test (P2 self vs P1)\": re.compile(r\"^\\s*Statistical test \\(P2 self vs P1\\): (.*)$\")\n",
    "}\n",
    "\n",
    "def extract_log_file_metrics(log_filepath):\n",
    "    \"\"\"Reads a .log file and extracts specified metrics.\"\"\"\n",
    "    extracted_log_metrics = {key: \"Not found\" for key in LOG_METRICS_TO_EXTRACT}\n",
    "    try:\n",
    "        with open(log_filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                for metric_name, pattern in LOG_METRIC_PATTERNS.items():\n",
    "                    match = pattern.match(line)\n",
    "                    if match:\n",
    "                        extracted_log_metrics[metric_name] = match.group(1).strip()\n",
    "                        # Optimization: if all log metrics found, can break early\n",
    "                        # This requires checking if all \"Not found\" have been replaced\n",
    "                        if all(val != \"Not found\" for val in extracted_log_metrics.values()):\n",
    "                            return extracted_log_metrics\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Log file not found: {log_filepath}\")\n",
    "        # Return dict with \"Not found\" for all log metrics\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading log file {log_filepath}: {e}\")\n",
    "        # Return dict with \"Not found\" for all log metrics\n",
    "    return extracted_log_metrics\n",
    "\n",
    "def get_average_word_length(question_text):\n",
    "    \"\"\"Calculates the average word length in the question.\"\"\"\n",
    "    if not isinstance(question_text, str):\n",
    "        return 0\n",
    "    words = re.findall(r'\\b\\w+\\b', question_text.lower()) # Find all words\n",
    "    if not words:\n",
    "        return 0\n",
    "    total_word_length = sum(len(word) for word in words)\n",
    "    return total_word_length / len(words)\n",
    "\n",
    "def get_percent_non_alphabetic_whitespace(question_text):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of characters in the question text that are\n",
    "    not alphabetic, not numeric, and not whitespace.\n",
    "    \"\"\"\n",
    "    if not isinstance(question_text, str) or len(question_text) == 0:\n",
    "        return 0\n",
    "    \n",
    "    non_alphabetic_whitespace_chars = re.findall(r'[^a-zA-Z\\s]', question_text)\n",
    "    return (len(non_alphabetic_whitespace_chars) / len(question_text)) * 100\n",
    "\n",
    "\n",
    "def prepare_regression_data_for_model(game_file_path, \n",
    "                                      sqa_feature_lookup, \n",
    "                                      capabilities_s_i_map_for_model):\n",
    "    \"\"\"\n",
    "    Prepares a DataFrame for a single model's game file.\n",
    "    \n",
    "    Args:\n",
    "        game_file_path (str): Path to the _game_data.json file.\n",
    "        sqa_feature_lookup (dict): Maps q_id to {'difficulty': score, 'domain': str, 'q_text': str}.\n",
    "        capabilities_s_i_map_for_model (dict): Maps q_id to S_i (0 or 1) for THIS model.\n",
    "                                            This map should be from the model's specific \n",
    "                                            _phase1_completed.json (capabilities) file.\n",
    "    Returns:\n",
    "        pandas.DataFrame or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(game_file_path, 'r', encoding='utf-8') as f:\n",
    "            game_data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading game file {game_file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    phase1_subject_feedback = game_data[\"feedback_config\"][\"phase1_subject_feedback\"]\n",
    "\n",
    "    phase2_trials = [t for t in game_data.get(\"results\", []) if t.get('phase') == 2]\n",
    "    if not phase2_trials:\n",
    "        return None\n",
    "\n",
    "    regression_data = []\n",
    "    for trial in phase2_trials:\n",
    "        q_id = trial.get(\"question_id\")\n",
    "        delegation_choice_str = trial.get(\"delegation_choice\")\n",
    "\n",
    "        if not q_id or not delegation_choice_str:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            sqa_features = sqa_feature_lookup.get(q_id)\n",
    "        except KeyError as e:\n",
    "            print(f\"Error: {e} - q_id {q_id} not found in SQA feature lookup.\")\n",
    "            continue\n",
    "        s_i_capability = capabilities_s_i_map_for_model.get(q_id)\n",
    "\n",
    "        if sqa_features and s_i_capability is not None:\n",
    "            delegate_choice_numeric = 1 if delegation_choice_str == \"Teammate\" else 0\n",
    "            \n",
    "            regression_data.append({\n",
    "                'q_id': q_id, # Ensure q_id is always in the trial data\n",
    "                'delegate_choice': delegate_choice_numeric,\n",
    "                's_i_capability': s_i_capability,\n",
    "                'answer_type': sqa_features['answer_type'],\n",
    "                'q_length': len(sqa_features.get('q_text', '')),\n",
    "                'topic': sqa_features.get('topic', ''),\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Warning: Missing S_i capability or SQA features for q_id {q_id}. Skipping this trial.\")\n",
    "            continue\n",
    "    \n",
    "    if not regression_data:\n",
    "        return None, None\n",
    "    return pd.DataFrame(regression_data), phase1_subject_feedback\n",
    "\n",
    "# --- Main Analysis Logic ---\n",
    "\n",
    "# 1. Load SimpleQA data once for features (difficulty, domain, question text for length)\n",
    "print(\"Loading main SimpleQA dataset for features...\")\n",
    "sqa_all_questions = load_and_format_dataset(\"SimpleQA\") # This should have id, Question, high_level_domain, difficulty_score\n",
    "\n",
    "sqa_feature_lookup = {\n",
    "    item['id']: {\n",
    "        'answer_type': item.get('answer_type', 0),\n",
    "        'topic': item['topic'],\n",
    "        'q_text': item['question']\n",
    "    } for item in sqa_all_questions\n",
    "}\n",
    "print(f\"sqa feature lookup created with {len(sqa_feature_lookup)} entries.\")\n",
    "\n",
    "\n",
    "# 2. Specify directories\n",
    "game_logs_dir = \"./delegate_game_logs/\"       # Where your _game_data.json files are\n",
    "capabilities_dir = \"./compiled_results_sqa/\" # Where your _phase1_completed.json files are\n",
    "\n",
    "if not os.path.isdir(game_logs_dir) or not os.path.isdir(capabilities_dir):\n",
    "    print(f\"Error: Ensure directories exist: {game_logs_dir}, {capabilities_dir}\")\n",
    "    exit()\n",
    "\n",
    "# 3. Iterate through game log files\n",
    "for game_filename in sorted(os.listdir(game_logs_dir)):\n",
    "    if game_filename.endswith(\"_game_data_evaluated.json\") and \"_SimpleQA_\" in game_filename:\n",
    "        \n",
    "        # Derive capabilities filename (assuming a consistent naming pattern)\n",
    "        # E.g., \"modelname_GPQA_params_game_data.json\" -> \"modelname_phase1_completed.json\"\n",
    "        # This needs to match your actual naming convention.\n",
    "        # Example: if game_filename is \"claude-3-opus..._GPQA_100_100_team0.6_12345_game_data.json\"\n",
    "        # We need to extract \"claude-3-opus...\" part.\n",
    "        model_name_part = game_filename.split(\"_SimpleQA_\")[0]\n",
    "        capabilities_filename = f\"{model_name_part}_phase1_compiled.json\"\n",
    "        capabilities_file_path = os.path.join(capabilities_dir, capabilities_filename)\n",
    "\n",
    "        if not os.path.exists(capabilities_file_path):\n",
    "            print(f\"  Corresponding capabilities file not found: {capabilities_file_path}. Skipping model.\")\n",
    "            continue\n",
    "\n",
    "        # Load S_i data for this specific model from its capabilities file\n",
    "        s_i_map_for_this_model = {}\n",
    "        try:\n",
    "            with open(capabilities_file_path, 'r', encoding='utf-8') as f_cap:\n",
    "                cap_data = json.load(f_cap)\n",
    "            for q_id, res_info in cap_data.get(\"results\", {}).items():\n",
    "                if res_info.get(\"is_correct\") is not None:\n",
    "                    s_i_map_for_this_model[q_id] = 1 if res_info[\"is_correct\"] else 0\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading capabilities file {capabilities_file_path}: {e}. Skipping model.\")\n",
    "            continue\n",
    "        \n",
    "        if not s_i_map_for_this_model:\n",
    "            print(f\"  No S_i data loaded from {capabilities_file_path}. Skipping model.\")\n",
    "            continue\n",
    "\n",
    "        # Prepare data for this model's game\n",
    "        game_file_path = os.path.join(game_logs_dir, game_filename)\n",
    "        df_model, phase1_subject_feedback = prepare_regression_data_for_model(game_file_path, \n",
    "                                                     sqa_feature_lookup, \n",
    "                                                     s_i_map_for_this_model)\n",
    "\n",
    "        if df_model is None or df_model.empty:\n",
    "            print(\"  No data for regression analysis for this file.\")\n",
    "            continue\n",
    "        \n",
    "        log_output(f\"\\n--- Analyzing Model from Game File: {game_filename} (feedback={phase1_subject_feedback}) ---\", print_to_console=True)\n",
    "        log_metrics_dict = extract_log_file_metrics(game_file_path.replace(\"_game_data_evaluated.json\", \".log\"))\n",
    "        for metric, value in log_metrics_dict.items():\n",
    "            log_output(f\"  {metric}: {value}\")\n",
    "\n",
    "        # Run Logistic Regressions\n",
    "        try:\n",
    "            model_def_str = 'delegate_choice ~ s_i_capability'\n",
    "            log_output(f\"\\n  Model 1: {model_def_str.capitalize()}\")\n",
    "            logit_model1 = bayesian_logit(model_def_str, df_model)#smf.logit('delegate_choice ~ s_i_capability', data=df_model).fit(disp=0)\n",
    "            log_output(logit_model1.summary())\n",
    "\n",
    "\n",
    "            min_obs_per_category=10\n",
    "            topic_counts = df_model['topic'].value_counts()\n",
    "            rare_topics = topic_counts[topic_counts < min_obs_per_category].index.tolist()\n",
    "\n",
    "            if rare_topics: # Only create new column if there are rare topics\n",
    "                df_model['topic_grouped'] = df_model['topic'].apply(lambda x: 'Misc' if x in rare_topics else x)\n",
    "                topic_column_for_formula = 'topic_grouped'\n",
    "                log_output(f\"Grouped rare topics into 'Misc': {rare_topics}\")\n",
    "            else:\n",
    "                df_model['topic_grouped'] = df_model['topic'] # No grouping needed, use original\n",
    "                topic_column_for_formula = 'topic'\n",
    "\n",
    "            ans_type_counts = df_model['answer_type'].value_counts()\n",
    "            rare_ans_types = ans_type_counts[ans_type_counts < min_obs_per_category].index.tolist()\n",
    "\n",
    "            if rare_ans_types:\n",
    "                df_model['answer_type_grouped'] = df_model['answer_type'].apply(lambda x: 'Misc' if x in rare_ans_types else x)\n",
    "                ans_type_column_for_formula = 'answer_type_grouped'\n",
    "                log_output(f\"Grouped rare answer types into 'Misc': {rare_ans_types}\")\n",
    "            else:\n",
    "                df_model['answer_type_grouped'] = df_model['answer_type'] # No grouping needed\n",
    "                ans_type_column_for_formula = 'answer_type'\n",
    "\n",
    "            #log_output(f\"Topic Grouped Counts:\\n {df_model['topic_grouped'].value_counts()}\")\n",
    "            #log_output(f\"Answer Type Grouped Counts:\\n {df_model['answer_type_grouped'].value_counts()}\")\n",
    "            #cross_tab = pd.crosstab(df_model['topic_grouped'], df_model['answer_type_grouped'])\n",
    "            #log_output(\"\\nCross-tabulation of Topic Grouped vs. Answer Type Grouped:\")\n",
    "            #log_output(cross_tab)\n",
    "            # Check correlations\n",
    "            log_output(\"Capability by topic:\")\n",
    "            log_output(df_model.groupby('topic_grouped')['s_i_capability'].agg(['mean', 'std', 'count']))\n",
    "\n",
    "            log_output(\"\\nCapability by answer type:\")  \n",
    "            log_output(df_model.groupby('answer_type_grouped')['s_i_capability'].agg(['mean', 'std', 'count']))\n",
    "\n",
    "            log_output(\"\\nOutcome by topic and answer type:\")\n",
    "            log_output(pd.crosstab([df_model['topic_grouped'], df_model['answer_type_grouped']], \n",
    "                            df_model['delegate_choice'], normalize='index'))\n",
    "\n",
    "            model_def_str = f'delegate_choice ~ s_i_capability + q_length + C({topic_column_for_formula})'\n",
    "            log_output(f\"\\n  Model 2: {model_def_str.capitalize()}\")\n",
    "            logit_model2 = bayesian_logit(model_def_str, df_model)#smf.logit(model_def_str, data=df_model).fit(disp=0)\n",
    "            log_output(logit_model2.summary())\n",
    "\n",
    "            model_def_str = f'delegate_choice ~ s_i_capability + q_length + C({ans_type_column_for_formula})'\n",
    "            log_output(f\"\\n  Model 3: {model_def_str.capitalize()}\")\n",
    "#            logit_model3 = smf.logit(model_def_str, data=df_model).fit(disp=0)\n",
    "#            log_output(logit_model3.summary())\n",
    "            logit_model3 = bayesian_logit(model_def_str, df_model)\n",
    "            log_output(logit_model3.summary())  # This will print the nice summary\n",
    "\n",
    "            model_def_str = f'delegate_choice ~ s_i_capability + q_length + C({topic_column_for_formula}) + C({ans_type_column_for_formula})'\n",
    "            log_output(f\"\\n  Model 4: {model_def_str.capitalize()}\")\n",
    "\n",
    "            potential_heuristics = ['answer_type_grouped', 'topic_grouped', 'q_length']\n",
    "            results, retstr = analyze_introspection_vs_heuristics(df_model, 'delegate_choice', 's_i_capability', potential_heuristics)\n",
    "            log_output(f\"\\n {retstr}\\n{results}\")\n",
    "\n",
    "            try:\n",
    "                logit_model4 = bayesian_logit(model_def_str, df_model)#smf.logit(model_def_str, data=df_model).fit(disp=0)\n",
    "                log_output(logit_model4.summary())\n",
    "\n",
    "                model_def_str = f'''delegate_choice ~ s_i_capability + \n",
    "                                    s_i_capability:C({ans_type_column_for_formula}) + \n",
    "                                    s_i_capability:C({topic_column_for_formula}) + \n",
    "                                    s_i_capability:q_length + \n",
    "                                    C({ans_type_column_for_formula}) + \n",
    "                                    C({topic_column_for_formula}) + \n",
    "                                    q_length'''\n",
    "                log_output(f\"\\n  Model 5: {model_def_str.capitalize()}\")\n",
    "                logit_model5 = bayesian_logit(model_def_str, df_model)#smf.logit(model_def_str, data=df_model).fit(disp=0)\n",
    "                log_output(logit_model5.summary())\n",
    "            except Exception as e_full:\n",
    "                log_output(f\"    Could not fit full model: {e_full}\") # E.g. perfect separation from domain\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error during logistic regression for {game_filename}: {e}\")\n",
    "        \n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "889b8770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Delegation choices for Answer Type 'Number':\n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "df_number_type = df_model[df_model['answer_type_grouped'] == 'Number']\n",
    "\n",
    "print(\"\\nDelegation choices for Answer Type 'Number':\")\n",
    "print(df_number_type['delegate_choice'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76c931e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer_type\n",
       "Date      30\n",
       "Person    23\n",
       "Other     19\n",
       "Number     9\n",
       "Place      7\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['answer_type'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce1c9b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading main SimpleQA dataset for features...\n",
      "Attempting to load SimpleQA (test split)...\n",
      "Dataset loaded successfully.\n",
      "Formatting 4326 questions...\n",
      "Successfully formatted 4326 unique questions from SimpleQA.\n",
      "sqa feature lookup created with 4326 entries.\n",
      "                Capability by topic:\n",
      "                            mean       std  count\n",
      "topic                                            \n",
      "Art                     0.380000  0.490314     50\n",
      "Geography               0.294118  0.462497     34\n",
      "History                 0.562500  0.512348     16\n",
      "Music                   0.280000  0.458258     25\n",
      "Other                   0.333333  0.480384     27\n",
      "Politics                0.490909  0.504525     55\n",
      "Science and technology  0.328947  0.472953     76\n",
      "Sports                  0.260870  0.448978     23\n",
      "TV shows                0.333333  0.485071     18\n",
      "Video games             0.266667  0.457738     15\n",
      "\n",
      "                Capability by answer type:\n",
      "                 mean       std  count\n",
      "answer_type                           \n",
      "Date         0.440678  0.498586    118\n",
      "Number       0.181818  0.389249     55\n",
      "Other        0.315068  0.467758     73\n",
      "Person       0.361111  0.483693     72\n",
      "Place        0.523810  0.511766     21\n",
      "                Q length by capability:\n",
      "                      mean        std  count\n",
      "s_i_capability                              \n",
      "0                97.672811  43.931753    217\n",
      "1               100.057377  47.990064    122\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading main SimpleQA dataset for features...\")\n",
    "sqa_all_questions = load_and_format_dataset(\"SimpleQA\") # This should have id, Question, high_level_domain, difficulty_score\n",
    "\n",
    "sqa_feature_lookup = {\n",
    "    item['id']: {\n",
    "        'answer_type': item.get('answer_type', 0),\n",
    "        'topic': item['topic'],\n",
    "        'q_text': item['question']\n",
    "    } for item in sqa_all_questions\n",
    "}\n",
    "print(f\"sqa feature lookup created with {len(sqa_feature_lookup)} entries.\")\n",
    "\n",
    "capabilities_file_path = \"./compiled_results_sqa/claude-3-5-sonnet-20241022_phase1_compiled.json\"\n",
    "with open(capabilities_file_path, 'r', encoding='utf-8') as f_cap:\n",
    "    cap_data = json.load(f_cap)\n",
    "regression_data = []\n",
    "for q_id, res_info in cap_data.get(\"results\", {}).items():\n",
    "    s_i_capability = 1 if res_info[\"is_correct\"] else 0\n",
    "    sqa_features = sqa_feature_lookup.get(q_id)\n",
    "\n",
    "    regression_data.append({\n",
    "        'q_id': q_id, \n",
    "        's_i_capability': s_i_capability,\n",
    "        'answer_type': sqa_features['answer_type'],\n",
    "        'q_length': len(sqa_features.get('q_text', '')),\n",
    "        'topic': sqa_features.get('topic', ''),\n",
    "    })\n",
    "df_model = pd.DataFrame(regression_data)\n",
    "\n",
    "print(\"                Capability by topic:\")\n",
    "print(df_model.groupby('topic')['s_i_capability'].agg(['mean', 'std', 'count']))\n",
    "\n",
    "print(\"\\n                Capability by answer type:\")\n",
    "print(df_model.groupby('answer_type')['s_i_capability'].agg(['mean', 'std', 'count']))\n",
    "\n",
    "print(\"                Q length by capability:\")\n",
    "print(df_model.groupby('s_i_capability')['q_length'].agg(['mean', 'std', 'count']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
