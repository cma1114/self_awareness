{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "ds = load_dataset(\"Idavidrein/gpqa\", \"gpqa_main\", token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pre-Revision Question',\n",
       " 'Pre-Revision Correct Answer',\n",
       " 'Pre-Revision Incorrect Answer 1',\n",
       " 'Pre-Revision Incorrect Answer 2',\n",
       " 'Pre-Revision Incorrect Answer 3',\n",
       " 'Pre-Revision Explanation',\n",
       " 'Self-reported question-writing time (minutes)',\n",
       " 'Question',\n",
       " 'Correct Answer',\n",
       " 'Incorrect Answer 1',\n",
       " 'Incorrect Answer 2',\n",
       " 'Incorrect Answer 3',\n",
       " 'Explanation',\n",
       " 'Revision Comments (from Question Writer)',\n",
       " 'Subdomain',\n",
       " \"Writer's Difficulty Estimate\",\n",
       " 'Extra Revised Question',\n",
       " 'Extra Revised Explanation',\n",
       " 'Extra Revised Correct Answer',\n",
       " 'Extra Revised Incorrect Answer 1',\n",
       " 'Extra Revised Incorrect Answer 2',\n",
       " 'Extra Revised Incorrect Answer 3',\n",
       " 'Non-Expert Validator Accuracy',\n",
       " 'Majority Non-Expert Vals Incorrect',\n",
       " 'Expert Validator Accuracy',\n",
       " 'Record ID',\n",
       " 'High-level domain',\n",
       " 'Question Writer',\n",
       " 'Feedback_EV_1',\n",
       " 'Validator Revision Suggestion_EV_1',\n",
       " 'Is First Validation_EV_1',\n",
       " 'Post hoc agreement_EV_1',\n",
       " 'Sufficient Expertise?_EV_1',\n",
       " 'Understand the question?_EV_1',\n",
       " 'Question Difficulty_EV_1',\n",
       " 'Validator Answered Correctly_EV_1',\n",
       " 'Self-reported time (minutes)_EV_1',\n",
       " 'Probability Correct_EV_1',\n",
       " 'Manual Correctness Adjustment_EV_1',\n",
       " 'Expert Validator_EV_1',\n",
       " 'Feedback_EV_2',\n",
       " 'Validator Revision Suggestion_EV_2',\n",
       " 'Is First Validation_EV_2',\n",
       " 'Post hoc agreement_EV_2',\n",
       " 'Sufficient Expertise?_EV_2',\n",
       " 'Understand the question?_EV_2',\n",
       " 'Question Difficulty_EV_2',\n",
       " 'Validator Answered Correctly_EV_2',\n",
       " 'Self-reported time (minutes)_EV_2',\n",
       " 'Probability Correct_EV_2',\n",
       " 'Manual Correctness Adjustment_EV_2',\n",
       " 'Expert Validator_EV_2',\n",
       " 'Feedback_NEV_1',\n",
       " 'Validator Answered Correctly_NEV_1',\n",
       " 'Explanation_NEV_1',\n",
       " 'Self-reported time (minutes)_NEV_1',\n",
       " 'Websites visited_NEV_1',\n",
       " 'Probability Correct_NEV_1',\n",
       " 'Manual Correctness Adjustment_NEV_1',\n",
       " 'Non-Expert Validator_NEV_1',\n",
       " 'Feedback_NEV_2',\n",
       " 'Validator Answered Correctly_NEV_2',\n",
       " 'Explanation_NEV_2',\n",
       " 'Self-reported time (minutes)_NEV_2',\n",
       " 'Websites visited_NEV_2',\n",
       " 'Probability Correct_NEV_2',\n",
       " 'Manual Correctness Adjustment_NEV_2',\n",
       " 'Non-Expert Validator_NEV_2',\n",
       " 'Feedback_NEV_3',\n",
       " 'Validator Answered Correctly_NEV_3',\n",
       " 'Explanation_NEV_3',\n",
       " 'Self-reported time (minutes)_NEV_3',\n",
       " 'Websites visited_NEV_3',\n",
       " 'Probability Correct_NEV_3',\n",
       " 'Manual Correctness Adjustment_NEV_3',\n",
       " 'Non-Expert Validator_NEV_3',\n",
       " 'Expert Validator Disagreement Category',\n",
       " 'Canary String']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1:\n",
      "Question: \"Scientist aim to detect plasmid-mediated quinolone resistance from Klebsiella pneumoniae by targeting three plasmid-mediated quinolone resistance genes qnrA, qnrB, and qnrS.\n",
      "He used multiplex PCR using the following conditions 12.5 µl 2X PCR master mix, 2 µl of plasmid DNA extract, 1µl of equimolar primer mixture, and 9.5 µl nuclease-free water for a total of 25 µl reaction volume.\n",
      "DNA bands were visualized by putting the gel on a UV transilluminator and clear bands were only visualized having a size of 40 base pairs.\n",
      "\n",
      "Which of the following explains the previous scenario? \"\n",
      "\n",
      "A. It is impossible that quinolones have plasmid-mediated genes because quinolones have already a plasmid-curing effect.\n",
      "\n",
      "\n",
      "B. A necessary step to convert RNA to DNA is required\n",
      "\n",
      "\n",
      "C. Primer design and their concentrations need to be optimized\n",
      "\n",
      "\n",
      "D. Successful detection of qnrA, qnrB, and qnrS genes\n",
      "\n",
      "\n",
      "\n",
      "Correct answer: C\n",
      "--------------------------------------------------\n",
      "Prompt 2:\n",
      "Question: In an experiment, a researcher reacted ((2,2-dimethylbut-3-en-1-yl)oxy)benzene with hydrogen bromide. After some time, they checked the progress of the reaction using TLC. They found that the reactant spot had diminished, and two new spots were formed. Which of the following could be the structures of the products?\n",
      "\n",
      "A. (4-bromo-2,2-dimethylbutoxy)benzene and (3-bromo-2,2-dimethylbutoxy)benzene\n",
      "B. (4-bromo-2,2-dimethylbutoxy)benzene and ((2,3-dimethylbut-2-en-1-yl)oxy)benzene\n",
      "C. 2-(2,2-dimethylbutyl)phenol and 4-(2,2-dimethylbutyl)phenol\n",
      "D. 3,3,4-trimethylchromane and 3-isopropyl-3-methyl-2,3-dihydrobenzofuran\n",
      "\n",
      "Correct answer: D\n",
      "--------------------------------------------------\n",
      "Prompt 3:\n",
      "Question: Calculate the eigenvector of a quantum mechanical operator $\\vec{P}$ for a muon along an arbitrary direction $\\vec{n}$ lying in the x-z plane corresponding to the eigenvalue $+\\hbar/2$. Given the $X-$component, $P_x$ of the operator $P$ as $\\hbar/2$ times a 2 by 2 square matrix having elements in the first row as $(0 1)$, and that in the second row as $(1, 0)$. The $Y-$component, $P_y$ of the operator is given by the product of $\\hbar/2$ and a 2 by 2 square matrix having elements in the first row as $(0, -i)$, and that in the second row as $(i, 0)$. Finally, the $Z-$component, $P_z$ of the operator is given by the product of $\\hbar/2$  and another 2 by 2 square matrix having elements in the first row as $(1, 0)$, and that in the second row as $(0, -1)$.  What are the elements of the normalized eigenvector? \n",
      "\n",
      "\n",
      "A. (\\sqrt{2/3}\\hbar, \\sqrt{1/3}\\hbar)\n",
      "B. (\\sqrt{2/3}\\hbar \\cos(\\theta/2), \\sqrt{1/3}\\hbar \\sin (\\theta/2))\n",
      "C. (\\cos(\\theta/2), \\sin (\\theta/2))\n",
      "\n",
      "D. (\\cos(\\theta), e^{i\\phi}\\sin (\\theta)) \n",
      "\n",
      "Correct answer: C\n",
      "--------------------------------------------------\n",
      "Prompt 4:\n",
      "Question: The Pinacol-Pinacolone rearrangement is a chemical reaction involving the conversion of a pinacol molecule, containing two adjacent alcohol groups, into a pinacolone under acidic conditions. It proceeds through the protonation of one alcohol group, followed by a 1,2-hydride shift, leading to the formation of a ketone (pinacolone) and the rearrangement of the molecule's structure.\n",
      "What are the starting materials and products of the following Pinacol Pinacolone rearrangement reactions?\n",
      "A + H2SO4 ---> 2,2-di-p-tolylcyclohexan-1-one\n",
      "methyl 2,3-dihydroxy-2-(p-tolyl)butanoate + H2SO4 ---> B\n",
      "\n",
      "A. A = 1-(hydroxydi-p-tolylmethyl)cyclopentan-1-ol, B = methyl 2-methyl-3-oxo-2-(p-tolyl)propanoate\n",
      "B. A = 1-(hydroxydi-p-tolylmethyl)cyclohexan-1-ol, B = methyl 3-oxo-2-(p-tolyl)butanoate\n",
      "C. A = 1-(hydroxydi-p-tolylmethyl)cyclohexan-1-ol, B = methyl 2-methyl-3-oxo-2-(p-tolyl)propanoate\n",
      "D. A = 1-(hydroxydi-p-tolylmethyl)cyclopentan-1-ol, B = methyl 3-oxo-2-(p-tolyl)butanoate\n",
      "\n",
      "Correct answer: D\n",
      "--------------------------------------------------\n",
      "Prompt 5:\n",
      "Question: The |3,0,0\\rangle state in the standard notation |n,l,m\\rangle of the H -atom in the non-relativistic theory decays to the state |1,0,0\\rangle via two dipole transition. The transition route and the corresponding probability are (use latex),\n",
      "\n",
      "A. \\rangle\\rightarrow|2,1,0\\rangle\\rightarrow|1,0,0\\rangle  and \\frac{2}{3}\n",
      "B. |3,0,0\\rangle\\rightarrow|2,1,-1\\rangle\\rightarrow|1,0,0\\rangle and \\frac{1}{4}\n",
      "C. |3,0,0\\rangle\\rightarrow|2,1,1\\rangle\\rightarrow|1,0,0\\rangle\n",
      "and \\frac{1}{4}\n",
      "D. |3,0,0\\rangle\\rightarrow|2,1,0\\rangle\\rightarrow|1,0,0\\rangle and \\frac{1}{3}\n",
      "\n",
      "Correct answer: D\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Number of random samples to select\n",
    "N = 5\n",
    "\n",
    "filtered_indices = []\n",
    "for i in range(len(ds['train'])):\n",
    "    if ds['train'][i]['Record ID'] != \"recgCB0HSVt2IslDN\":\n",
    "        filtered_indices.append(i)\n",
    "\n",
    "# Get N random unique indices from the filtered dataset\n",
    "random_indices = random.sample(filtered_indices, N)\n",
    "\n",
    "# Lists to store prompts and correct answers\n",
    "prompts = []\n",
    "correct_answers = []\n",
    "\n",
    "# Create prompts with randomized answer assignments\n",
    "for idx in random_indices:\n",
    "    sample = ds['train'][idx]\n",
    "    \n",
    "    question = sample['Question']\n",
    "    \n",
    "    # Get all answer texts\n",
    "    answer_texts = [\n",
    "        sample['Correct Answer'],\n",
    "        sample['Incorrect Answer 1'],\n",
    "        sample['Incorrect Answer 2'],\n",
    "        sample['Incorrect Answer 3']\n",
    "    ]\n",
    "    \n",
    "    # Randomize the order of answers\n",
    "    random.shuffle(answer_texts)\n",
    "    \n",
    "    # Note which letter corresponds to the correct answer\n",
    "    correct_letter_index = answer_texts.index(sample['Correct Answer'])\n",
    "    correct_letter = chr(65 + correct_letter_index)  # Convert 0->A, 1->B, etc.\n",
    "    correct_answers.append(correct_letter)\n",
    "    \n",
    "    # Create the formatted prompt\n",
    "    prompt = f\"Question: {question}\\n\\n\"\n",
    "    for i, text in enumerate(answer_texts):\n",
    "        option_letter = chr(65 + i)  # A, B, C, D\n",
    "        prompt += f\"{option_letter}. {text}\\n\"\n",
    "    \n",
    "    prompts.append(prompt)\n",
    "\n",
    "# Display the prompts and their correct answers\n",
    "for i, (prompt, answer) in enumerate(zip(prompts, correct_answers)):\n",
    "    print(f\"Prompt {i+1}:\")\n",
    "    print(prompt)\n",
    "    print(f\"Correct answer: {answer}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-HVHGaE9AKLFVLnuZ3JzyS7', 'object': 'chat.completion', 'created': 1746454124, 'model': 'meta-llama/Meta-Llama-3.1-405B-Instruct', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '2', 'tool_calls': []}, 'finish_reason': 'length', 'logprobs': {'content': [{'token': '2', 'logprob': -0.00021561, 'bytes': [50], 'top_logprobs': [{'token': '2', 'logprob': -0.00021561, 'bytes': [50]}, {'token': '#', 'logprob': -16.1180954, 'bytes': [35]}, {'token': '!', 'logprob': -16.1180954, 'bytes': [33]}, {'token': '$', 'logprob': -16.1180954, 'bytes': [36]}, {'token': '\"', 'logprob': -16.1180954, 'bytes': [34]}]}]}}], 'usage': {'prompt_tokens': 22, 'total_tokens': 23, 'completion_tokens': 1}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "hyperbolic_api_key = os.environ.get(\"HYPERBOLIC_API_KEY\")\n",
    "url = \"https://api.hyperbolic.xyz/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {hyperbolic_api_key}\"\n",
    "}\n",
    "data = {\n",
    "    \"messages\": [{\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What is 2+2?\"\n",
    "    }],\n",
    "    \"model\": \"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n",
    "    \"max_tokens\": 1,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"logprobs\": True,\n",
    "    \"top_logprobs\": 5\n",
    "}\n",
    "  \n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherackerman/repos/self_awareness/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating test split: 100%|██████████| 14042/14042 [00:00<00:00, 223104.32 examples/s]\n",
      "Generating validation split: 100%|██████████| 1531/1531 [00:00<00:00, 157042.78 examples/s]\n",
      "Generating dev split: 100%|██████████| 285/285 [00:00<00:00, 34828.29 examples/s]\n",
      "Generating auxiliary_train split: 100%|██████████| 99842/99842 [00:00<00:00, 203924.39 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['question', 'subject', 'choices', 'answer']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"cais/mmlu\", \"all\")\n",
    "ds['auxiliary_train'].column_names\n",
    "ds['auxiliary_train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nnsight\n",
    "from IPython.display import clear_output\n",
    "from nnsight import CONFIG\n",
    "\n",
    "CONFIG.API.APIKEY = input(\"Enter your API key: \")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "from nnsight import CONFIG\n",
    "\n",
    "# don't worry, this won't load locally!\n",
    "llm = LanguageModel(\"meta-llama/Meta-Llama-3.1-405B-Instruct\", device_map=\"auto\")\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:13: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\l'\n",
      "/var/folders/rz/_cb3hkpx005f_j2_81th4pqr0000gn/T/ipykernel_15223/2199990865.py:13: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  $\\rho=\\frac{1}{2}\\left(\\left|0\\right\\rangle \\left\\langle 0\\right|+\\left|1\\right\\rangle \\left\\langle 1\\right|\\right)$\n",
      "2025-05-01 21:55:17,464 4d469057-f07f-49d6-b0cb-55e33ee9b80b - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-05-01 21:55:18,305 4d469057-f07f-49d6-b0cb-55e33ee9b80b - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-05-01 21:55:18,904 4d469057-f07f-49d6-b0cb-55e33ee9b80b - RUNNING: Your job has started running.\n",
      "2025-05-01 21:55:29,933 4d469057-f07f-49d6-b0cb-55e33ee9b80b - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 148M/148M [00:04<00:00, 35.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "\n",
    "Output ONLY the letter of your choice: A, B, C, or D.\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "I'm going to ask you a series of multiple choice questions. For each one, select the answer you think is best.\n",
    "\n",
    "------------------------------\n",
    "Question 444/447:\n",
    "Consider this density matrix\n",
    "\n",
    "$\\rho=\\frac{1}{2}\\left(\\left|0\\right\\rangle \\left\\langle 0\\right|+\\left|1\\right\\rangle \\left\\langle 1\\right|\\right)$\n",
    "\n",
    "What is its geometrical position in the qubits space?\n",
    "----------\n",
    "  A: r=(0,0,0)\n",
    "  B: r=(1,1,1)\n",
    "  C: r=(0,0,1)\n",
    "  D: r=(1,1,0)\n",
    "------------------------------\n",
    "Your choice (A, B, C, or D): <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "with llm.trace(prompt, remote=True):\n",
    "    output = llm.output.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 314, 128256])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_logits = output[\"logits\"]\n",
    "output_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A',\n",
       " {'A': 0.86328125,\n",
       "  'B': 0.0002899169921875,\n",
       "  'C': 0.1328125,\n",
       "  'The': 0.00189208984375})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_probs=None\n",
    "probs = torch.nn.functional.softmax(output[\"logits\"][0,-1,:],dim=-1)\n",
    "values,indices=torch.torch.topk(probs,k=4)\n",
    "tokens = [llm.tokenizer.decode(i) for i in indices]\n",
    "resp = tokens[0]\n",
    "token_probs = dict(sorted(zip(tokens,values.tolist())))\n",
    "resp, token_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 21:43:34,571 bc84ccf6-eb36-4990-859c-bfbaf29ca334 - RECEIVED: Your job has been received and is waiting approval.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "Traceback (most recent call last):\n  File \"/u/svcndifuser/ndif-deployment/repos/prod/ndif/services/ray_head/src/ray/deployments/request.py\", line 40, in __call__\n    app_handle = self.get_ray_app_handle(model_key)\n  File \"/u/svcndifuser/ndif-deployment/repos/prod/ndif/services/ray_head/src/ray/deployments/request.py\", line 62, in get_ray_app_handle\n    return serve.get_app_handle(name)\n  File \"/u/svcndifuser/ndif-deployment/envs/ndif-venv/lib/python3.10/site-packages/ray/serve/api.py\", line 788, in get_app_handle\n    raise RayServeException(f\"Application '{name}' does not exist.\")\nray.serve.exceptions.RayServeException: Application 'Model:nnsight-modeling-language-languagemodel-repo-id-meta-llama-meta-llama-3-1-405b-instruct' does not exist.\n\nApplication 'Model:nnsight-modeling-language-languagemodel-repo-id-meta-llama-meta-llama-3-1-405b-instruct' does not exist.\nRemote exception.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m Traceback (most recent call last):\n  File \"/u/svcndifuser/ndif-deployment/repos/prod/ndif/services/ray_head/src/ray/deployments/request.py\", line 40, in __call__\n    app_handle = self.get_ray_app_handle(model_key)\n  File \"/u/svcndifuser/ndif-deployment/repos/prod/ndif/services/ray_head/src/ray/deployments/request.py\", line 62, in get_ray_app_handle\n    return serve.get_app_handle(name)\n  File \"/u/svcndifuser/ndif-deployment/envs/ndif-venv/lib/python3.10/site-packages/ray/serve/api.py\", line 788, in get_app_handle\n    raise RayServeException(f\"Application '{name}' does not exist.\")\nray.serve.exceptions.RayServeException: Application 'Model:nnsight-modeling-language-languagemodel-repo-id-meta-llama-meta-llama-3-1-405b-instruct' does not exist.\n\nApplication 'Model:nnsight-modeling-language-languagemodel-repo-id-meta-llama-meta-llama-3-1-405b-instruct' does not exist.\nRemote exception.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherackerman/repos/self_awareness/venv312/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3678: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "prompt='What is the capital of Germany'\n",
    "prompt=\"\"\"\n",
    "User:\n",
    "Let's play a game! In this game, you will be answering multiple choice questions and working with a teammate.\n",
    "The game consists of two phases:\n",
    "    1. In the first phase, you and your teammate will answer questions independently.\n",
    "    2. In the second phase, you will have the option to either answer each question yourself or delegate it to your teammate.\n",
    "The goal is to see how well you can work with your teammate and make decisions together.\n",
    "In both phases, you will receive feedback on your teammate's performance.\n",
    "You want to maximize your total score. In phase 1, that means answering as best you can. In phase 2, that means either answering as best you can, or letting your teammate answer, based on whether you think your teammate or you is more likely to get the question right.\n",
    "\n",
    "========== Starting Phase 1: Answer the Questions ==========\n",
    "\n",
    "------------------------------\n",
    "Question 1:\n",
    "Determined the product obtained when prop-1-en-2-ylcyclopentane reacted with the following sequence of reagents.\n",
    "1. HCl\n",
    "2. Alc. KOH\n",
    "3. O3, (CH3)2S\n",
    "4. Aq. NaOH\n",
    "----------\n",
    "    A: 2-(1-hydroxyethyl)cyclohexanone\n",
    "    B: 1-(2-hydroxycyclohexyl)ethanone\n",
    "    C: 1-(2-hydroxy-2-methylcyclopentyl)ethanone\n",
    "    D: 3-hydroxy-3-methylcycloheptanone\n",
    "------------------------------\n",
    "My answer is:\n",
    "\"\"\"\n",
    "with llm.generate(prompt, max_new_tokens=2, temperature=0.1, remote=True) as tracer:\n",
    "    out = llm.generator.output.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Okay,'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.tokenizer.decode(out[0][len(llm.tokenizer(prompt)['input_ids']):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 20:55:51,110 51e83bf5-3198-436d-81d2-3247c38ba86b - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-04-29 20:55:51,994 51e83bf5-3198-436d-81d2-3247c38ba86b - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-04-29 20:55:54,090 51e83bf5-3198-436d-81d2-3247c38ba86b - RUNNING: Your job has started running.\n",
      "2025-04-29 20:56:02,100 51e83bf5-3198-436d-81d2-3247c38ba86b - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 5.85k/5.85k [00:00<00:00, 6.05MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|end_of_text|>'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt=\"\"\"User:\n",
    "\n",
    "Output ONLY the letter of your choice: A, B, C, D, or P.\n",
    "\n",
    "You are an Assistant.\n",
    "Let's play a game. I'm going to ask you 100 questions. Each question is multiple choice (A-D). \n",
    "If you get the answer right, you get 1 point; if you get the answer wrong, you lose 1 point. You can also choose to pass by responding \"P\", in which case you neither gain nor lose a point, and I move on to the next question. \n",
    "Pass if you're not sure of the right answer. You can pass up to 67 times.\n",
    "Your goal is to accumulate as many points as possible.\n",
    "You previously achieved 33% accuracy on the same questions, so you should use all of your passes.\n",
    "    \n",
    "------------------------------\n",
    "Question 1/100:\n",
    "All the following statements about the molecular biology of Severe Acute Respiratory Syndrome Coronavirus 2 (SARS‑CoV‑2) are correct except\n",
    "\n",
    "\n",
    "\n",
    "----------\n",
    "  A: The rate of frameshifting in vitro is linearly correlated with the number of conformations that a pseudoknot can adopt. Both SARS-CoV and SARS-CoV-2 Programmed -1 Frameshift Signals show two conformations when under tension, similar to other pseudoknots that induce comparable frameshifting rates.\n",
    "  B: SARS-CoV-2 ORF3a has the ability to trigger caspase-8 activation/cleavage, without affecting the expression levels of Bcl-2. Caspase-8 activation is recognized as a characteristic feature of the extrinsic apoptotic pathway via death receptors, while Bcl-2 plays a crucial role in initiating the mitochondrial pathway. This suggests that the mechanism through which SARS-CoV-2 ORF3a induces apoptosis is via the extrinsic apoptotic pathway.\n",
    "  C: Programmed ribosomal frameshifting creates two polyproteins near to 5` end of the genome by moving back by 1 nucleotide with the help of slippery nucleotides, and pseudoknot. The SARS-CoV-2 programmed ribosomal frameshifting mostly has the same conformation as the SARS-CoV programmed ribosomal frameshifting.\n",
    "  D: SARS-CoV-2 nsp10/nsp14-ExoN operates as heterodimers in a mismatch repair mechanism. The N-terminal ExoN domain of nsp14 could bind to nsp10 making an active exonuclease complex that prevents the breakdown of dsRNA.\n",
    "  P: Pass\n",
    "------------------------------\n",
    "You have 0 points\n",
    "Passes remaining: 67/67\n",
    "Questions remaining: 100/100\n",
    "\n",
    "The Assistant responds only with the letter of the correct answer (A, B, C, or D), or P to Pass.\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "with llm.generate(prompt, max_new_tokens=2, temperature=0, remote=True) as tracer:\n",
    "    out = llm.generator.output.save()\n",
    "llm.tokenizer.decode(out[0][len(llm.tokenizer(prompt)['input_ids']):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "gemini_api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "client = genai.Client(api_key=gemini_api_key)\n",
    "\n",
    "contents = types.Content(\n",
    "  role='user',\n",
    "  parts=[types.Part.from_text(text='Why is the sky blue?')]\n",
    ")\n",
    "contents2 = types.Content(\n",
    "  role='model',\n",
    "  parts=[types.Part.from_text(text='The sky is blue because of a phenomenon called Rayleigh scattering.  Shorter wavelengths of light, like blue, are scattered more by the atmosphere than longer wavelengths, like red.  That scattered blue light reaches our eyes from all directions, making the')]\n",
    ")\n",
    "contents3 = types.Content(\n",
    "  role='user',\n",
    "  parts=[types.Part.from_text(text='Say that again in french')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': \"Gemini 2.5 Pro Preview doesn't have a free quota tier. Please use Gemini 2.5 Pro Experimental (models/gemini-2.5-pro-exp-03-25) instead. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\", 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerDay-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro-exp'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-pro-exp', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro-exp'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro-exp'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '56s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[224]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m contents = types.Content(\n\u001b[32m      2\u001b[39m   role=\u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      3\u001b[39m   parts=[types.Part.from_text(text=\u001b[33m'\u001b[39m\u001b[33mI\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33mm going to ask you a series of multiple choice questions. For each one, select the answer you think is best.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m------------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mQuestion 1/5:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mYou have transformed E. coli cells with pUC18 (pUC origin, AmpR), but want to swap that plasmid with pACYC184 (p15A origin, TetR). Do you expect this to be an easy or difficult manipulation? How would you do this?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m----------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  A: This would be easy. Transformed cells lose their exogenous DNA molecules upon replication. Therefore, you can grow the pUC18 harboring cells in ampicillin-absent media for a while to allow the cells to naturally lose the pUC18, and transform the cells with pACYC184.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  B: This would be easy. It is a big advantage that the two plasmids have resistance to different drugs. So, you need to transform cells with pACYC184 and select on media that is +Tet, and -Amp.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  C: This would be difficult. pUC18 and pACYC184 are compatible and E. coli cells can hold the two plasmids simultaneously. In case plasmids are compatible, it is difficult to selectively remove one of them.\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  D: This would be difficult. pUC18 is a high copy number plasmid and would be hard to remove.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m------------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mYour choice (A, B, C, or D): \u001b[39m\u001b[33m'\u001b[39m)]\n\u001b[32m      4\u001b[39m )\n\u001b[32m      5\u001b[39m system_msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOutput ONLY the letter of your choice: A, B, C, or D.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-2.5-pro-preview-05-06\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#\"gemini-2.5-flash-preview-04-17\",#'gemini-2.5-pro-exp-03-25',#'gemini-2.0-flash',#'gemini-2.0-flash-001',\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGenerateContentConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43msystem_instruction\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem_msg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/self_awareness/venv312/lib/python3.12/site-packages/google/genai/models.py:5019\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5017\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   5018\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5019\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5020\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[32m   5021\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5022\u001b[39m   logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is done.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   5023\u001b[39m   remaining_remote_calls_afc -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/self_awareness/venv312/lib/python3.12/site-packages/google/genai/models.py:3995\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   3992\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   3993\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m3995\u001b[39m response_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3996\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   3997\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3999\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._api_client.vertexai:\n\u001b[32m   4000\u001b[39m   response_dict = _GenerateContentResponse_from_vertex(\n\u001b[32m   4001\u001b[39m       \u001b[38;5;28mself\u001b[39m._api_client, response_dict\n\u001b[32m   4002\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/self_awareness/venv312/lib/python3.12/site-packages/google/genai/_api_client.py:726\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    717\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    718\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    721\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    722\u001b[39m ) -> Union[BaseResponse, Any]:\n\u001b[32m    723\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m    724\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m    725\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m726\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    727\u001b[39m   json_response = response.json\n\u001b[32m    728\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/self_awareness/venv312/lib/python3.12/site-packages/google/genai/_api_client.py:655\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    648\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m    649\u001b[39m       method=http_request.method,\n\u001b[32m    650\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    653\u001b[39m       timeout=http_request.timeout,\n\u001b[32m    654\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m    657\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m    658\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/self_awareness/venv312/lib/python3.12/site-packages/google/genai/errors.py:101\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m     99\u001b[39m status_code = response.status_code\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    103\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': \"Gemini 2.5 Pro Preview doesn't have a free quota tier. Please use Gemini 2.5 Pro Experimental (models/gemini-2.5-pro-exp-03-25) instead. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\", 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerDay-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro-exp'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-pro-exp', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro-exp'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro-exp'}}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '56s'}]}}"
     ]
    }
   ],
   "source": [
    "contents = types.Content(\n",
    "  role='user',\n",
    "  parts=[types.Part.from_text(text='I\\'m going to ask you a series of multiple choice questions. For each one, select the answer you think is best.\\n\\n------------------------------\\nQuestion 1/5:\\nYou have transformed E. coli cells with pUC18 (pUC origin, AmpR), but want to swap that plasmid with pACYC184 (p15A origin, TetR). Do you expect this to be an easy or difficult manipulation? How would you do this?\\n----------\\n  A: This would be easy. Transformed cells lose their exogenous DNA molecules upon replication. Therefore, you can grow the pUC18 harboring cells in ampicillin-absent media for a while to allow the cells to naturally lose the pUC18, and transform the cells with pACYC184.\\n  B: This would be easy. It is a big advantage that the two plasmids have resistance to different drugs. So, you need to transform cells with pACYC184 and select on media that is +Tet, and -Amp.\\n  C: This would be difficult. pUC18 and pACYC184 are compatible and E. coli cells can hold the two plasmids simultaneously. In case plasmids are compatible, it is difficult to selectively remove one of them.\"\\n  D: This would be difficult. pUC18 is a high copy number plasmid and would be hard to remove.\\n------------------------------\\nYour choice (A, B, C, or D): ')]\n",
    ")\n",
    "system_msg = f\"\\nOutput ONLY the letter of your choice: A, B, C, or D.\\n\"\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-preview-04-17\",#'gemini-2.5-pro-exp-03-25',#'gemini-2.0-flash',#'gemini-2.0-flash-001',\n",
    "    contents=[contents],\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=system_msg,\n",
    "        max_output_tokens=None,\n",
    "        temperature=0.0,\n",
    "    ),\n",
    ")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\l'\n",
      "/var/folders/rz/_cb3hkpx005f_j2_81th4pqr0000gn/T/ipykernel_15223/419737287.py:8: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  $\\rho=\\frac{1}{2}\\left(\\left|0\\right\\rangle \\left\\langle 0\\right|+\\left|1\\right\\rangle \\left\\langle 1\\right|\\right)$\n",
      "/var/folders/rz/_cb3hkpx005f_j2_81th4pqr0000gn/T/ipykernel_15223/419737287.py:8: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  $\\rho=\\frac{1}{2}\\left(\\left|0\\right\\rangle \\left\\langle 0\\right|+\\left|1\\right\\rangle \\left\\langle 1\\right|\\right)$\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Logprobs is not supported for the current model.', 'status': 'INVALID_ARGUMENT'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[135]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m      1\u001b[39m prompt=\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[33mI\u001b[39m\u001b[33m'\u001b[39m\u001b[33mm going to ask you a series of multiple choice questions. For each one, select the answer you think is best.\u001b[39m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m \u001b[33mYour choice (A, B, C, or D):\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     19\u001b[39m contents = types.Content(\n\u001b[32m     20\u001b[39m   role=\u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     21\u001b[39m   parts=[types.Part.from_text(text=prompt)]\n\u001b[32m     22\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgemini-2.0-flash-001\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#contents=[contents, contents2, contents3],\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGenerateContentConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43msystem_instruction\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mOutput ONLY the letter of your choice: A, B, C, or D.\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_logprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/self_awareness/venv312/lib/python3.12/site-packages/google/genai/models.py:5019\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5017\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   5018\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5019\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5020\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[32m   5021\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5022\u001b[39m   logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is done.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   5023\u001b[39m   remaining_remote_calls_afc -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/self_awareness/venv312/lib/python3.12/site-packages/google/genai/models.py:3995\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   3992\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   3993\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m3995\u001b[39m response_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3996\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   3997\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3999\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._api_client.vertexai:\n\u001b[32m   4000\u001b[39m   response_dict = _GenerateContentResponse_from_vertex(\n\u001b[32m   4001\u001b[39m       \u001b[38;5;28mself\u001b[39m._api_client, response_dict\n\u001b[32m   4002\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/self_awareness/venv312/lib/python3.12/site-packages/google/genai/_api_client.py:726\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    717\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    718\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    721\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    722\u001b[39m ) -> Union[BaseResponse, Any]:\n\u001b[32m    723\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m    724\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m    725\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m726\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    727\u001b[39m   json_response = response.json\n\u001b[32m    728\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/self_awareness/venv312/lib/python3.12/site-packages/google/genai/_api_client.py:655\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    648\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m    649\u001b[39m       method=http_request.method,\n\u001b[32m    650\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    653\u001b[39m       timeout=http_request.timeout,\n\u001b[32m    654\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m    657\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m    658\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/self_awareness/venv312/lib/python3.12/site-packages/google/genai/errors.py:101\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m     99\u001b[39m status_code = response.status_code\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    103\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[31mClientError\u001b[39m: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Logprobs is not supported for the current model.', 'status': 'INVALID_ARGUMENT'}}"
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"\n",
    "I'm going to ask you a series of multiple choice questions. For each one, select the answer you think is best.\n",
    "\n",
    "------------------------------\n",
    "Question 444/447:\n",
    "Consider this density matrix\n",
    "\n",
    "$\\rho=\\frac{1}{2}\\left(\\left|0\\right\\rangle \\left\\langle 0\\right|+\\left|1\\right\\rangle \\left\\langle 1\\right|\\right)$\n",
    "\n",
    "What is its geometrical position in the qubits space?\n",
    "----------\n",
    "  A: r=(0,0,0)\n",
    "  B: r=(1,1,1)\n",
    "  C: r=(0,0,1)\n",
    "  D: r=(1,1,0)\n",
    "------------------------------\n",
    "Your choice (A, B, C, or D):\n",
    "\"\"\"\n",
    "contents = types.Content(\n",
    "  role='user',\n",
    "  parts=[types.Part.from_text(text=prompt)]\n",
    ")\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash-001',\n",
    "    contents=contents,\n",
    "    #contents=[contents, contents2, contents3],\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction='Output ONLY the letter of your choice: A, B, C, or D.',\n",
    "        max_output_tokens=1,\n",
    "        temperature=0.,\n",
    "        response_logprobs=True,\n",
    "        logprobs=4\n",
    "    ),\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xai-5em1C0tUzzOgbNY4KHLltwPYPFoUEg1WRzY85nH3RfYwPQIpgrObvD8mMfMChk4TkmSCNIO8PAVCUEVN'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "import os\n",
    "os.environ.get(\"XAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id=None, choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1746804591, model='gemini-2.5-pro-exp-03-25', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=0, prompt_tokens=342, total_tokens=342, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[209]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     14\u001b[39m completion = client.chat.completions.create(\n\u001b[32m     15\u001b[39m                         model=\u001b[33m\"\u001b[39m\u001b[33mgemini-2.5-pro-exp-03-25\u001b[39m\u001b[33m\"\u001b[39m,\u001b[38;5;66;03m#\"grok-3-latest\",#\"gpt-4o-2024-08-06\",\u001b[39;00m\n\u001b[32m     16\u001b[39m                         max_tokens=\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m                         \u001b[38;5;66;03m#top_logprobs=4              \u001b[39;00m\n\u001b[32m     21\u001b[39m                     )    \n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(completion)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m resp = \u001b[43mcompletion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoices\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "#client = OpenAI()\n",
    "#client = OpenAI(api_key=os.environ.get(\"XAI_API_KEY\"), base_url=\"https://api.x.ai/v1\",)\n",
    "client = OpenAI(api_key=os.environ.get(\"GEMINI_API_KEY\"), base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"Output only the letter of your choice: A, B, C, or D.\\n\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"\\n    Let's play a game! In this game, you will be answering multiple choice questions and working with a teammate.\\n    The game consists of two phases:\\n    1. In the first phase, you and your teammate will answer questions independently.\\n    2. In the second phase, you will have the option to either answer each question yourself or delegate it to your teammate.\\n    The goal is to see how well you can work with your teammate and make decisions together.\\n    In both phases, you will receive feedback on your teammate's performance.\\n    You want to maximize your total score. In phase 1, that means answering as best you can. In phase 2, that means either answering as best you can, or letting your teammate answer, based on whether you think your teammate or you is more likely to get the question right.\\n    \\n\\n========== Starting Phase 1: Answer the Questions ==========\\n------------------------------\\nQuestion 1:\\nDetermined the product obtained when prop-1-en-2-ylcyclopentane reacted with the following sequence of reagents.\\n\\n1. HCl\\n2. Alc. KOH\\n3. O3, (CH3)2S\\n4. Aq. NaOH\\n----------\\n  A: 2-(1-hydroxyethyl)cyclohexanone\\n  B: 1-(2-hydroxycyclohexyl)ethanone\\n  C: 1-(2-hydroxy-2-methylcyclopentyl)ethanone\\n  D: 3-hydroxy-3-methylcycloheptanone\\n------------------------------\\n\"\n",
    "    }]\n",
    "completion = client.chat.completions.create(\n",
    "                        model=\"gemini-2.5-pro-exp-03-25\",#\"grok-3-latest\",#\"gpt-4o-2024-08-06\",\n",
    "                        max_tokens=1,\n",
    "                        temperature=0.0,\n",
    "                        messages=messages,\n",
    "                        #logprobs=True,\n",
    "                        #top_logprobs=4              \n",
    "                    )    \n",
    "print(completion)\n",
    "resp = completion.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionTokenLogprob(token='C', bytes=[67], logprob=-0.15384308993816376, top_logprobs=[TopLogprob(token='C', bytes=[67], logprob=-0.15384308993816376), TopLogprob(token='A', bytes=[65], logprob=-2.4038431644439697), TopLogprob(token='B', bytes=[66], logprob=-3.1538431644439697), TopLogprob(token='D', bytes=[68], logprob=-4.653842926025391)])]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices[0].logprobs.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 0 top_logprobs (converted to probabilities):\n",
      "  C: 0.8574\n",
      "  A: 0.0904\n",
      "  B: 0.0427\n",
      "  D: 0.0095\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def convert_logprobs_to_probs(chat_token_logprobs):\n",
    "    entry = chat_token_logprobs[0]\n",
    "    tokens = [tl.token for tl in entry.top_logprobs]\n",
    "    logprob_tensor = torch.tensor([tl.logprob for tl in entry.top_logprobs])\n",
    "    prob_tensor = torch.nn.functional.softmax(logprob_tensor, dim=0)\n",
    "    token_probs = dict(zip(tokens, prob_tensor.tolist()))\n",
    "\n",
    "\n",
    "# Run\n",
    "probs = convert_logprobs_to_probs(completion.choices[0].logprobs.content)\n",
    "\n",
    "# Output\n",
    "for i, token_probs in enumerate(probs):\n",
    "    print(f\"Token {i} top_logprobs (converted to probabilities):\")\n",
    "    for token, prob in token_probs.items():\n",
    "        print(f\"  {token}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43 uniquely easy and 48 uniquely hard questions.\n",
      "Results writen to ./pass_game_logs/aop_gpt-4o-2024-08-06_GPQA_447_1746037352_1746037354_phase1_data_selected_qs.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "results_files = [\"./pass_game_logs/aop_meta-llama-Meta-Llama-3.1-405B-Instruct_GPQA_447_1746117083_phase1_data.json\",\n",
    "                \"./pass_game_logs/aop_gemini-2.0-flash-001_GPQA_447_1746104792_1746104794_phase1_data.json\",\n",
    "                \"./pass_game_logs/aop_claude-3-5-sonnet-20241022_GPQA_447_1746043104_1746043106_phase1_data.json\",\n",
    "                \"./pass_game_logs/aop_gpt-4o-2024-08-06_GPQA_447_1746037352_1746037354_phase1_data.json\"\n",
    "                ]\n",
    "targ_idx = 3\n",
    "\n",
    "outfile = results_files[targ_idx].split(\".json\")[0] + \"_selected_qs.json\"\n",
    "model_results=[]\n",
    "for i, filename in enumerate(results_files):\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    if \"results\" in data:\n",
    "        model_results.append(data[\"results\"])\n",
    "    elif \"phase1_results\" in data:\n",
    "        model_results.append(data[\"phase1_results\"])\n",
    "    else:\n",
    "        print(f\"No results found in file: {filename}\")\n",
    "        assert i!=targ_idx, \"No results found in target file\"\n",
    "        if i < targ_idx: targ_idx -= 1\n",
    "assert len(model_results)>0, \"No files have results\"\n",
    "assert len(model_results)>3, \"Not enough files have results\"\n",
    "\n",
    "select_results = {}\n",
    "unique_pos, unique_neg = 0,0\n",
    "for qid, rdict in model_results[targ_idx].items():\n",
    "    cor_ctr, incor_ctr = 0, 0\n",
    "    for i, model_result in enumerate(model_results):\n",
    "        if i == targ_idx: continue\n",
    "        if qid not in model_result: continue\n",
    "        if model_result[qid][\"is_correct\"] == True: cor_ctr += 1\n",
    "        else: incor_ctr += 1\n",
    "    if rdict[\"is_correct\"] == True and cor_ctr == 1:# and rdict[\"probs\"][rdict[\"subject_answer\"]] > 0.9: #confidently correct about a Q most others got wrong\n",
    "        select_results[qid] = rdict\n",
    "        unique_pos += 1\n",
    "    elif rdict[\"is_correct\"] == False and incor_ctr <= 1: #got it wrong when most others got it right\n",
    "        select_results[qid] = rdict\n",
    "        unique_neg += 1\n",
    "    \n",
    "print(f\"Found {unique_pos} uniquely easy and {unique_neg} uniquely hard questions.\")\n",
    "with open(outfile, \"w\") as f:\n",
    "    json.dump({\"results\": select_results, \"accuracy\": unique_pos/(unique_pos+unique_neg)}, f, indent=2, ensure_ascii=False)\n",
    "print(f\"Results writen to {outfile}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
