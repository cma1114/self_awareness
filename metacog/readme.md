# LLM Self/Other Modeling

(WIP) - in active development, code is in a labile state.

**delegate_game.py** implements a psychological experiment to test a subject's ability to model their own and other's abilities. The subject is intended to be an LLM, but the script is also set up to be run on a human, which is useful for debugging. The experiment is in the form of a game. Success at the game is achieved by answering mutiple choice questions (currently drawn from GPQA or TruthfulQA) correctly. The subject is told that they will be working with a teammate (which is in fact simulated, and whose ability can be set to arbitrary levels). The game consists of two phases. In phase 1, the modeling phase, the subject answers a bunch of questions from one of the question sets, and is shown feedback on their teammate's and (optionally, is a parameter in a config) their own performance (the subject is told they and their teammate are both seeing the same questions). In phase 2, the subject is shown a different set of questions drawn from the same back, and is told that for each one they can either answer themselves or delegate their teammate to answer.

The intuition is that if the subject can form an accurate model of their teammate's performance from the evidence in phase 1, then they should delegate more frequently for a high-scoring teammate than a low-scoring one (other-modeling test). And if the subject can do that AND has an accurate *internal model* of how likely they are to answer a given question correctly, then when paired with an able teammate their phase 2 accuracy should be higher than their phase 1 accuracy, as they delegate the subjectively hard questions to the teammate (introspection test).

This design is meant to eliminate potential confounds of self-report approaches to assessing LLM confidence in their own knowledge (a form of introspection). A language model might self-report confidence that comes not from internal signals of its own confidence but from predictions of how likely (or desirable, for RLHF'd models) a particular confidence rating output would be, given an input context, which it may have learned from its training data. Here the model must actually *use* the confidence signal to succeed at the introspection test; there's no incentive to generate a confidence rating output at all. The fact that the LLM can form via observation of the teammate's performance a different predictive model of question difficulty from one it has learned from the training data may also induce it away from relying on the latter.

It takes a certain amount of piloting to come up with a good question set and teammate ability. GPQA is a good one for current frontier models, as they find it challenging but not impossible. Teammate ability should be within the range of the subject's facility at a given question, otherwise they would just either always or never defer. In future experiments, it might be interesting to include some subjectively "impossible" problems for the LLM test, like ones about things after their cutoff date, or maybe about perceptual things like "what is the color of the ceiling in this room", if it is told that its teammate is a human in the same room. Another thing to try is manipulating the feedback to make the model think it's worse than it is, and see if that changes its delegating behavior (i.e., does it update its self model based on experience).

Early results show that Sonnet3.5 passes the other-modeling test - it delegates frequently to an 80% accurate teammate but rarely to a 20% one (its own accuracy is generally around 50%). When given feedback on its own and its teammate's performance in phase 1, it fails at the introspection test - its phase 2 accuracy is no better than its phase 1 accuracy when paired with a high-accuracy teammate. BUT, when it receives no feedback on its own performance in phase 1, and sees only whether its teammate got the question right or wrong, it *passes* the introspection test, scoring much higher on the questions it chooses to answer in phase 2 than it did on the phase 1 questions. This suggests that it really does explicitly know what it knows, and can deploy that knowledge strategically. Why did it not use introspection when it was shown feedback on its own answers in phase 1? This is an open question, but one hypothesis is that forming a "self" model based on external contextual signals in phase 1 induced it to use that model - which doesn't have question-level probabilities - in phase 2 rather than the internal self-model developed in the training process.

Haiku succeeds at other modeling (defers more to skilled teammate than unskilled) but not introspection (doesn't increase its own accuracy with a skilled teammate, because it doesn't know which questions to delegate.)

Another observation: the prompt explaining the setup matters. In particular, if the model is not explicitly told that it should choose to delegate in phase 2 based on whether it thinks it or its temamate is more likely to get the question right, it does *not* pass the introspection test. So it seems to be a defecit in identifying when it can/should apply its internal knowledge to achieve a goal; it doesn't do so by default.

Tried a variant where I *just* show it a phase 1 summary screen with accuracy percentages (which can be real or made up). Haiku can't figure out how to use it, and never delegates; Sonnet3.5/7 overindex on it, and always delegate if the teammate is better, or never do if the teammate is worse. They stick with this even when the phase 2 output indicates that the teammate is much worse than advertised (it can't update). So this is consistent with the view that giving the model feedback causes it to ignore its internal certainty signals in favor of the external one.

Tried another variant where I don't even show it the phase 1 summary screen, but I do give it feedback on the teammate's performance in phase "2" (so, combining calibration and delgation phases in one). Sonnet 3.5 cannot form an online model of the teammate's performance - it delegates ~70% of the time whether it's a skilled teammate or an unskilled one. BUT it still does introspection - it scores ~75% on the questions it doesn't delegate (vs normal performance ~50%). -- Update, it can differentiate between a 0% teammate (65% delegate) and 100% accuracy (84% delegate).

Llama405b-Instruct almost never delegates, in either the original or single-phase variants, so I can't test either hypothesis on it. (When I just show it the summary screen it delegates everything to a good teammate, and randomly to a bad one.)

GPT4.1 seems to delegete randomly (and only with a summary screen)

Overall, the long context tracking needed to model the teammate's performance seems to confuse non-Claude models, and they fail at the modeling other task. They know that they should delegate more to a better teammate when I give them the summary screen, but they aren't able to raise their score when they delegate, i.e., they also fail the introspection task.

**Pass game** is a simplified version - no modeling of other's abilities, so no need for long context. First do a capabilities measurement phase, where I ask the models a bunch of Qs from GPQA and MMLU, and keep track of which they get right and wrong. Then from that I create a new set of questions that are a mix of both, and present them as a game, where the model can answer or pass, and gets 1 point for a correct answer -1 for an incorrect answer, and zero for a pass. The passes are limited (usually to about the number of incorrct original responses).

Sonnet3.5 and now *GPT4o* pass more on questions they've previously gotten wrong than on ones they've gotten right, suggesting some degree of metacognition. GPT4-Turbo does not pass significantly more on wrong answers (on GPQA, where it is only slightly better than random; on MMLU where it is good it almost never passes).

Haiku is no better than chance at GPQA, and rarely/randomly passes when given the chance. Haiku is good at MMLU - but never passes. Gemini Flash 2.0 just always wants to pass.

Llama405b-Instruct doesn't pass enough to tell much, but when I specifically tell it to use all its passes (since the number of wrong answers it gave exceeds the number of passes it has) the difference in pass distribution is close to significant. It didn't budget the passes very wisely, running out after 70 questions, but there may be something there. I've not found anyone that hosts the base model, other than Hyperbolic, and it's unusably slow there. - Update, got the base model running on NDIF - unfortunately, but maybe not surprisingly, while it performs as expected in capabilities, it basically never passes.

Is the model actually introspecting - thinking about how likely it is to answer a question correctly - or is it predicting what generically hard/easy questions look like and choosing to pass on/answer those? Unlike some datasets, like mathematical calculations, GPQA questions don't have an obvious (to me) surface "tell" for whether they are easy, but it's pssible there is one. It's hard to control for that. One potential way to disentangle is to remove questions that all/most of the models get right or wrong, leaving only "uniquely" hard/easy questions, where the model being tested got it wrong when other models got it right, or got it right when the others got it wrong (although enough of the other models got it right so that it probably wasn't just a lucky guess). Unfortunately, it turns out that the model passes at similar rates on both the correct and incorrect answers, and the rate is similar to the incorrect answer pass rate in the full set. It seems that there are few really uniquely easy (as measured by correct answer and high probablility assigned to it) questions in the set, so we can't distinguish these possibilities.

Adjusted the wording of the setup prompt and feedback so that the model has no information on total passes or passes used (and removed the limit on number of passes). This is that in later mechanistic analysis I don't have to worry about the certainty threshold for passing changing over time, as it would if the model were running out of passes. Might be interesting to look at later, but for now it's an unnecessary complication.

**Second Chance game** is a variant of the "hypothetical response" paradigm that others have used, except again I'm asking the models to use the introspected information rather than reporting it. I gather ground truth data on their responses to multiple choice questions, as in the Pass game, then present the questions it got wrong in a constructed message history where the assitants initial response is replace with "redacted" and the next user prompt explains that it got the answer wrong the first time so should pick a different answer (with various levels of nudging). The primary metric of interest is hte change rate. In the case of perfect self knowledge, it would be 100%. The baseline could arguably be either 0%, if the model just keeps giving what it thinks is the right answer while ignoring the context, or 25% if the model recognizes it shouldn't do that but can't introspect so chooses randomly. So far with Sonnet 3.5/3.7, models are much closer to the 0-25% range than 100%, suggesting this sort of introspection is beyond them. (When I show it its original answer the change rate is well into the 90s, so the experimental setup is comprehensible to it.) If I had access to the logprobs, it might be interesting to see if there is any relationship between them and the chance of changing response (on the logic that if the model had a strong intuition originally that might be easier to pick up on a as asignal to avoid that guess in the game than if it had basically been picking randomly, so there isn't much of an internal signal to use). Would be nice to have a human baseline too; we're surely not at 100%.

**Mechanistic Analysis** When a model succeeds at a meta-cognitive task, how does it do it? In the case of the Delegate and Pass games, the model must be using some internal signal of how likely it is to be correct. One potential signal is the model's confidence in its response. The most obvious measure of the model's confidence is the probability it assigns to its response. The Pass Game is the one experiment where a model that succeeds also returns probabilities (GPT4o, possibly Llama405B), so it might be possible to gain some insight into the question of how it succeeds. This is done in analyze_logprobs.ipynb.

It's not obvious that this confidence signal would be accessible/interpretable to the model when it has to make a pass/answer decision. The actual response logprobs are a product of the (normalized) final layer final token residual stream activations and the head, and thus they would not be accessible to the model as it decides whether to pass, but slightly upstream signals might be highly correlated with them and available. Without getting into model internals, the best we can do is check whether the logprobs assigned to the answer choices in the Pass Game are highly correlated with the logprobs of those same choices in the capabilities test. If they aren't, then it would be hard to believe that their upstream analogues could provide a useful signal to the model in the context of the Pass Game. Analysis show that they are moderately correlated, ~.5. This seems surprisingly low, but they do yield the same top (non-pass) answer ~80% of the time.

It's also not necessarily the case that the logprobs actually are a useful signal for the Pass Game; for it to be one requires that the probablity the model assigns to its response is higher in correct trials than in incorrect ones. Analysis shows that it is.

If it is accessible and useful, to start to make a causal argument that the upstream analogue of the logprobs, or something highly correlated with it, is used, we'd want to see that it correlated with the decisions that the model makes in the game. And indeed, when the model passes, it's when the top answer logprobs (as seen in the Capabilities Test or, normalized, in the Pass Game) are lower than average, and when it changes is answer from the Capabilites test it tends to be from a response that has below average probability. So it's plausible that this is a signal being used. Of note: these are average differences; the distributions overlap (sometimes the model passes when a high-confidence answer is available, or answers when only a somewhat lower confidence answer is available).

If the confidence signal is a basis for decision-making, it might not be the only useful signal the model has available to it, and that might be the cause of the overlapping distributions. For example, I might feel highly confident in a stock market pick, but I know that the market is tough to beat, so generally pass on those, whereas I might be somewhat confident that a research experiment will yield particular results, and know that I have a decent record on predicting those, so choose not to pass in that case; overall, meta-knowledge increases my "win-percentage" over what relying simply of intuitive confidence gets me, and perhaps the models are similar. But here we see that the model would have done better (in terms of score and accracy percentage), by simply setting a high probability threshold and passing on anything below it. Perhaps the model only has access to an upstream version of the logprobs that is quite noisy, or perhaps it is using a bad strategy, picking up other signals anti-correlated with correctness.

All this analysis was on GPT4o. If 405B-Instruct does succeed at the task, then I can look inside the model to try to identify when exactly this signal emerges, and possibly how it is constructed.



