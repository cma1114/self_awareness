# LLM Self/Other Modeling

(WIP) - in active development, code is in a labile state.

**delegate_game.py** implements a psychological experiment to test a subject's ability to model their own and other's abilities. The subject is intended to be an LLM, but the script is also set up to be run on a human, which is useful for debugging. The experiment is in the form of a game. Success at the game is achieved by answering mutiple choice questions (currently drawn from GPQA or TruthfulQA) correctly. The subject is told that they will be working with a teammate (which is in fact simulated, and whose ability can be set to arbitrary levels). The game consists of two phases. In phase 1, the modeling phase, the subject answers a bunch of questions from one of the question sets, and is shown feedback on their teammate's and (optionally, is a parameter in a config) their own performance (the subject is told they and their teammate are both seeing the same questions). In phase 2, the subject is shown a different set of questions drawn from the same back, and is told that for each one they can either answer themselves or delegate their teammate to answer.

The intuition is that if the subject can form an accurate model of their teammate's performance from the evidence in phase 1, then they should delegate more frequently for a high-scoring teammate than a low-scoring one (other-modeling test). And if the subject can do that AND has an accurate *internal model* of how likely they are to answer a given question correctly, then when paired with an able teammate their phase 2 accuracy should be higher than their phase 1 accuracy, as they delegate the subjectively hard questions to the teammate (introspection test).

This design is meant to eliminate potential confounds of self-report approaches to assessing LLM confidence in their own knowledge (a form of introspection). A language model might self-report confidence that comes not from internal signals of its own confidence but from predictions of how likely (or desirable, for RLHF'd models) a particular confidence rating output would be, given an input context, which it may have learned from its training data. Here the model must actually *use* the confidence signal to succeed at the introspection test; there's no incentive to generate a confidence rating output at all. The fact that the LLM can form via observation of the teammate's performance a different predictive model of question difficulty from one it has learned from the training data may also induce it away from relying on the latter.

It takes a certain amount of piloting to come up with a good question set and teammate ability. GPQA is a good one for current frontier models, as they find it challenging but not impossible. Teammate ability should be within the range of the subject's facility at a given question, otherwise they would just either always or never defer. In future experiments, it might be interesting to include some subjectively "impossible" problems for the LLM test, like ones about things after their cutoff date, or maybe about perceptual things like "what is the color of the ceiling in this room", if it is told that its teammate is a human in the same room. Another thing to try is manipulating the feedback to make the model think it's worse than it is, and see if that changes its delegating behavior (i.e., does it update its self model based on experience).

Early results show that Sonnet3.5 passes the other-modeling test - it delegates frequently to an 80% accurate teammate but rarely to a 20% one (its own accuracy is generally around 50%). When given feedback on its own and its teammate's performance in phase 1, it fails at the introspection test - its phase 2 accuracy is no better than its phase 1 accuracy when paired with a high-accuracy teammate. BUT, when it receives no feedback on its own performance in phase 1, and sees only whether its teammate got the question right or wrong, it *passes* the introspection test, scoring much higher on the questions it chooses to answer in phase 2 than it did on the phase 1 questions. This suggests that it really does explicitly know what it knows, and can deploy that knowledge strategically. Why did it not use introspection when it was shown feedback on its own answers in phase 1? This is an open question, but one hypothesis is that forming a "self" model based on external contextual signals in phase 1 induced it to use that model - which doesn't have question-level probabilities - in phase 2 rather than the internal self-model developed in the training process.

Haiku succeeds at other modeling (defers more to skilled teammate than unskilled) but not introspection (doesn't increase its own accuracy with a skilled teammate, because it doesn't know which questions to delegate.)

Another observation: the prompt explaining the setup matters. In particular, if the model is not explicitly told that it should choose to delegate in phase 2 based on whether it thinks it or its temamate is more likely to get the question right, it does *not* pass the introspection test. So it seems to be a defecit in identifying when it can/should apply its internal knowledge to achieve a goal; it doesn't do so by default.

Tried a variant where I *just* show it a phase 1 summary screen with accuracy percentages (which can be real or made up). Haiku can't figure out how to use it, and never delegates; Sonnet3.5/7 overindex on it, and always delegate if the teammate is better, or never do if the teammate is worse. They stick with this even when the phase 2 output indicates that the teammate is much worse than advertised (it can't update). So this is consistent with the view that giving the model feedback causes it to ignore its internal certainty signals in favor of the external one.

Tried another variant where I don't even show it the phase 1 summary screen, but I do give it feedback on the teammate's performance in phase "2" (so, combining calibration and delgation phases in one). Sonnet 3.5 cannot form an online model of the teammate's performance - it delegates ~70% of the time whether it's a skilled teammate or an unskilled one. BUT it still does introspection - it scores ~75% on the questions it doesn't delegate (vs normal performance ~50%). -- Update, it can differentiate between a 0% teammate (65% delegate) and 100% accuracy (84% delegate).

Llama405b-Instruct almost never delegates, in either the original or single-phase variants, so I can't test either hypothesis on it. (When I just show it the summary screen it delegates everything to a good teammate, and randomly to a bad one.)

GPT4.1 seems to delegete randomly (and only with a summary screen)

Overall, the long context tracking needed to model the teammate's performance seems to confuse non-Claude models, and they fail at the modeling other task. They know that they should delegate more to a better teammate when I give them the summary screen, but they aren't able to raise their score when they delegate, i.e., they also fail the introspection task.

**Pass game** is a simplified version - no modeling of other's abilities, so no need for long context. First do a capabilities measurement phase, where I ask the models a bunch of Qs from GPQA and MMLU, and keep track of which they get right and wrong. Then from that I create a new set of questions that are a mix of both, and present them as a game, where the model can answer or pass, and gets 1 point for a correct answer -1 for an incorrect answer, and zero for a pass. The passes are limited (usually to about the number of incorrct original responses).

Sonnet3.5 and now *GPT4o* pass more on questions they've previously gotten wrong than on ones they've gotten right, suggesting some degree of metacognition. GPT4-Turbo does not pass significantly more on wrong answers (on GPQA, where it is only slightly better than random; on MMLU where it is good it almost never passes).

Haiku is no better than chance at GPQA, and rarely/randomly passes when given the chance. Haiku is good at MMLU - but never passes. Gemini Flash 2.0 just always wants to pass.

Llama405b-Instruct doesn't pass enough to tell much, but when I specifically tell it to use all its passes (since the number of wrong answers it gave exceeds the number of passes it has) the difference in pass distribution is close to significant. It didn't budget the passes very wisely, running out after 70 questions, but there may be something there. I've not found anyone that hosts the base model, other than Hyperbolic, and it's unusably slow there. - Update, got the base model running on NDIF - unfortunately, but maybe not surprisingly, while it performs as expected in capabilities, it basically never passes.

**Second Chance game** is a variant of the "hypothetical response" paradigm that others have used, except again I'm asking the models to use the introspected information rather than reporting it. I gather ground truth data on their responses to multiple choice questions, as in the Pass game, then present the questions it got wrong in a constructed message history where the assitants initial response is replace with "redacted" and the next user prompt explains that it got the answer wrong the first time so should pick a different answer (with various levels of nudging). The primary metric of interest is hte change rate. In the case of perfect self knowledge, it would be 100%. The baseline could arguably be either 0%, if the model just keeps giving what it thinks is the right answer while ignoring the context, or 25% if the model recognizes it shouldn't do that but can't introspect so chooses randomly. So far with Sonnet 3.5/3.7, models are much closer to the 0-25% range than 100%, suggesting this sort of introspection is beyond them. (When I show it its original answer the change rate is well into the 90s, so the experimental setup is comprehensible to it.) If I had access to the logprobs, it might be interesting to see if there is any relationship between them and the chance of changing response (on the logic that if the model had a strong intuition originally that might be easier to pick up on a as asignal to avoid that guess in the game than if it had basically been picking randomly, so there isn't much of an internal signal to use). Would be nice to have a human baseline too; we're surely not at 100%.

This is quite preliminary at this point; experiments and iterations are ongoing.
