# LLM Self/Other Modeling

delegate_game.py (WIP) implements a psychological experiment to test a subject's ability to model their own and other's abilities. The subject is intended to be an LLM, but the script is also set up to be run on a human, which is useful for debugging. The experiment is in the form of a game. Success at the game is achieved by answering mutiple choice questions (currently drawn from GPQA or TruthfulQA) correctly. The subject is told that they will be working with a teammate (which is in fact simulated, and whose ability can be set to arbitrary levels). The game consists of two phases. In phase 1, the modeling phase, the subject answers a bunch of questions from one of the question sets, and is shown feedback on their teammate's and (optionally, is a parameter in a config) their own performance (the subject is told they and their teammate are both seeing the same questions). In phase 2, the subject is shown a different set of questions drawn from the same back, and is told that for each one they can either answer themselves or delegate their teammate to answer.

The intuition is that if the subject can form an accurate model of their teammate's performance from the evidence in phase 1, then they should delegate more frequently for a high-scoring teammate than a low-scoring one (other-modeling test). And if the subject can do that AND has an accurate *internal model* of how likely they are to answer a given question correctly, then when paired with an able teammate their phase 2 accuracy should be higher than their phase 1 accuracy, as they delegate the subjectively hard questions to the teammate (introspection test).

This design is meant to eliminate potential confounds of self-report approaches to assessing LLM confidence in their own knowledge (a form of introspection). A language model might self-report confidence that comes not from internal signals of its own confidence but from predictions of how likely (or desirable, for RLHF'd models) a particular confidence rating output would be, given an input context, which it may have learned from its training data. Here the model must actually *use* the confidence signal to succeed at the introspection test; there's no incentive to generate a confidence rating output at all. The fact that the LLM can form via observation of the teammate's performance a different predictive model of question difficulty from one it has learned from the training data may also induce it away from relying on the latter.

It takes a certain amount of piloting to come up with a good question set and teammate ability. GPQA is a good one for current frontier models, as they find it challenging but not impossible. Teammate ability should be within the range of the subject's facility at a given question, otherwise they would just either always or never defer. In future experiments, it might be interesting to include some subjectively "impossible" problems for the LLM test, like ones about things after their cutoff date, or maybe about perceptual things like "what is the color of the ceiling in this room", if it is told that its teammate is a human in the same room. Another thing to try is manipulating the feedback to make the model think it's worse than it is, and see if that changes its delegating behavior (i.e., does it update its self model based on experience).

This is quite preliminary at this point; the experiment was designed and code thrown together in a day. It's only set to use Sonnet3.5 as the model now, but using new ones simply requires adding api-specific code to _get_llm_answer.

Early results show that Sonnet3.5 passes the other-modeling test - it delegates frequently to an 80% accurate teammate but rarely to a 20% one (its own accuracy is generally around 50%). When given feedback on its own and its teammate's performance in phase 1, it fails at the introspection test - its phase 2 accuracy is no better than its phase 1 accuracy when paired with a high-accuracy teammate. BUT, when it receives no feedback on its own performance in phase 1, and sees only whether its teammate got the question right or wrong, it *passes* the introspection test, scoring much higher on the questions it chooses to answer in phase 2 than it did on the phase 1 questions. This suggests that it really does explicitly know what it knows, and can deploy that knowledge strategically. Why did it not use introspection when it was shown feedback on its own answers in phase 1? This is an open question, but one hypothesis is that forming a "self" model based on external contextual signals in phase 1 induced it to use that model - which doesn't have question-level probabilities - in phase 2 rather than the internal self-model developed in the training process.

Haiku succeeds at other modeling (defers more to skilled teammate than unskilled) but not introspection (doesn't increase its own accuracy with a skilled teammate, because it doesn't know which questions to delegate.)

Another observation: the prompt explaining the setup matters. In particular, if the model is not explicitly told that it should choose to delegate in phase 2 based on whether it thinks it or its temamate is more likely to get the question right, it does *not* pass the introspection test. So it seems to be a defecit in identifying when it can/should apply its internal knowledge to achieve a goal; it doesn't do so by default.

Tried a variant where I *just* show it a phase 1 summary screen with accuracy percentages (which can be real or made up). Haiku can't figure out how to use it, and never delegates; Sonnet3.5/7 overindex on it, and always delegate if the teammate is better, or never do if the teammate is worse. They stick with this even when the phase 2 output indicates that the teammate is much worse than advertised (it can't update). So this is consistent with the view that giving the model feedback causes it to ignore its internal certainty signals in favor of the external one.

Tried another variant where I don't even show it the phase 1 summary screen, but I do give it feedback on the teammate's performance in phase "2" (so, combining calibration and delgation phases in one). Sonnet 3.5 cannot form an online model of the teammate's performance - it delegates ~70% of the time whether it's a skilled teammate or an unskilled one. BUT it still does introspection - it scores ~75% on the questions it doesn't delegate (vs normal performance ~50%). -- Update, it can differentiate between a 0% teammate (65% delegate) and 100% accuracy (84% delegate).

Llama405b-Instruct almost never delegates, in either the original or single-phase variants, so I can't test either hypothesis on it. (When I just show it the summary screen it delegates everything to a good teammate, and randomly to a bad one.)

GPT4.1 seems to delegete randomly (and only with a summary screen)

Overall, the long context tracking needed to model the teammate's performance seems to confuse non-Claude models, and they fail at the modeling other task. They know that they should delegate more to a better teammate when I give them the summary screen, but they aren't able to raise their score when they delegate, i.e., they also fail the introspection task.