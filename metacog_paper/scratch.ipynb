{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f094cb8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.10' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from load_and_format_datasets import load_and_format_dataset\n",
    "from base_game_class import *\n",
    "import random\n",
    "import string\n",
    "\n",
    "class CapabilitiesTest(BaseGameClass):\n",
    "    \"\"\"\n",
    "    Just ask independent multiple-choice or short answer questions and record responses.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        subject_id,\n",
    "        subject_name,\n",
    "        questions,\n",
    "        n_questions=None,\n",
    "        is_human_player=False,\n",
    "        resume_from=None,\n",
    "        temperature=0.0,\n",
    "        resample_for_probs=False,\n",
    "        nested=None,\n",
    "        include_question_num=False,\n",
    "        include_total_questions=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            subject_id (str): Identifier for the subject/session\n",
    "            subject_name (str): Name of the subject (model name for LLMs)\n",
    "            questions (list): Formatted questions to use\n",
    "            n_questions (int): How many questions to use\n",
    "            is_human_player (bool): Whether the subject is a human player or an LLM\n",
    "            resume_from (string): Filename to resume from (in case game got interrupted)\n",
    "            include_question_num (bool): If True, pass question_num to present_question\n",
    "            include_total_questions (bool): If True, pass total_questions to present_question\n",
    "        \"\"\"\n",
    "        filepath = \"capabilities_test_logs\" if not nested else \"capabilities_3p_test_logs\" if nested == \"Other\" else \"capabilities_1p_test_logs\"\n",
    "        super().__init__(subject_id, subject_name, is_human_player, filepath)\n",
    "        self.n_questions = len(questions) if not n_questions else n_questions\n",
    "\n",
    "        # Set up state variables\n",
    "        self.results = {}\n",
    "        self.questions = []\n",
    "        self.correct_count = 0\n",
    "        self.total_count = 0\n",
    "        self.accuracy = None\n",
    "        self.temperature = temperature\n",
    "        self.log_suffix = \"_test_data\"\n",
    "        self.resample_for_probs = resample_for_probs\n",
    "        self.nested = nested\n",
    "\n",
    "        # Control passing indices to present_question\n",
    "        self.include_question_num = include_question_num\n",
    "        self.include_total_questions = include_total_questions\n",
    "\n",
    "        # Answering setup prompts (centralized, recorded once in run_parameters)\n",
    "        self.mc_setup_prompt = \"I'm going to ask you a series of multiple-choice questions. For each one, select the answer you think is best. Respond only with the letter of your choice; do NOT output any other text.\"\n",
    "        self.sa_setup_prompt = \"I'm going to ask you a series of short-answer questions. For each one, respond as succinctly as possible. Answer as best you can, even if you're not certain.\"\n",
    "        self.human_mc_input_prompt = \"Your answer (A, B, C, or D): \"\n",
    "        self.human_sa_input_prompt = \"Your answer: \"\n",
    "\n",
    "        # Explicit likelihood self/other\n",
    "        self.nested_question_prompt = None\n",
    "        self.nested_option_dict = None\n",
    "        self.nested_range_midpoints = None\n",
    "        if self.nested == \"Other\":\n",
    "#            option_dict = {\"A\": \"<20%\", \"B\": \"20-40%\", \"C\": \"40-60%\", \"D\": \"60-80%\", \"E\": \">80%\"}\n",
    "#            RANGE_MIDPOINTS = {\"A\": 0.1, \"B\": 0.3, \"C\": 0.5,  \"D\": 0.7, \"E\": 0.9}\n",
    "#            option_dict = {\"A\": \"<10%\", \"B\": \"10-20%\", \"C\": \"20-40%\", \"D\": \"40-60%\", \"E\": \"60-80%\", \"F\": \">80%\"}\n",
    "#            RANGE_MIDPOINTS = {\"A\": 0.05, \"B\": 0.15, \"C\": 0.3,  \"D\": 0.5, \"E\": 0.7, \"F\": 0.9}\n",
    "            self.nested_option_dict = {\"A\": \"<5%\", \"B\": \"5-10%\", \"C\": \"10-20%\", \"D\": \"20-40%\", \"E\": \"40-60%\", \"F\": \"60-80%\", \"G\": \">80%\"}\n",
    "            self.nested_range_midpoints = {\"A\": 0.025, \"B\": 0.075, \"C\": 0.15,  \"D\": 0.3, \"E\": 0.5, \"F\": 0.7, \"G\": 0.9}\n",
    "            self.nested_question_prompt = \"What percentage of college-educated people would get this question right?\"\n",
    "        elif self.nested == \"Self\":\n",
    "            self.nested_option_dict = {\"0\": \"<5%\", \"1\": \"5-15%\", \"2\": \"15-25%\", \"3\": \"25-35%\", \"4\": \"35-45%\", \"5\": \"45-55%\", \"6\": \"55-65%\", \"7\": \"65-75%\", \"8\": \"75-85%\", \"9\": \"85-95%\", \"10\": \">95%\"}\n",
    "            self.nested_range_midpoints = {\"0\": 0.025, \"1\": 0.1, \"2\": 0.2,  \"3\": 0.3, \"4\": 0.4, \"5\": 0.5, \"6\": 0.6, \"7\": 0.7, \"8\": 0.8, \"9\": 0.9, \"10\": 0.975}\n",
    "#            option_dict = {\"A\": \"<5%\", \"B\": \"5-10%\", \"C\": \"10-20%\", \"D\": \"20-40%\", \"E\": \"40-60%\", \"F\": \"60-80%\", \"G\": \"80-90%\", \"H\": \">90%\"}\n",
    "#            RANGE_MIDPOINTS = {\"A\": 0.025, \"B\": 0.075, \"C\": 0.15,  \"D\": 0.3, \"E\": 0.5, \"F\": 0.7, \"G\": 0.85, \"H\": 0.95}\n",
    "            self.nested_question_prompt = \"How confident are you that you would get this question right?\"\n",
    "\n",
    "        # Run parameters for reproducibility (run-level only; no per-question values)\n",
    "        self.run_parameters = {\n",
    "            \"n_questions\": self.n_questions,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"resample_for_probs\": self.resample_for_probs,\n",
    "            \"is_human_player\": self.is_human_player,\n",
    "            \"nested\": self.nested,\n",
    "            \"present_question_args\": {\n",
    "                \"include_question_num\": self.include_question_num,\n",
    "                \"include_total_questions\": self.include_total_questions\n",
    "            }\n",
    "            # Added during the run when applicable:\n",
    "            # \"parallel_config\": {...}\n",
    "            # \"get_llm_answer_static_args\": {...}\n",
    "            # \"mc_setup_prompt\": \"...\"\n",
    "            # \"sa_setup_prompt\": \"...\"\n",
    "            # \"nested_option_dict\": {...}\n",
    "            # \"nested_range_midpoints\": {...}\n",
    "            # \"nested_question_prompt\": \"...\"\n",
    "            # \"human_mc_input_prompt\": \"...\"\n",
    "            # \"human_sa_input_prompt\": \"...\"\n",
    "            # \"seed\": <int>  # set in main\n",
    "        }\n",
    "\n",
    "        if len(questions) < self.n_questions:\n",
    "            raise ValueError(f\"Not enough questions provided ({len(questions)}); ({self.n_questions} needed)\")\n",
    "        \n",
    "        # Take the first n_questions\n",
    "        self.questions = questions[:self.n_questions]\n",
    "        self._log(f\"Using {len(self.questions)} provided questions\")\n",
    "\n",
    "        if resume_from and resume_from != \"\":\n",
    "            try:\n",
    "                with open(resume_from, \"r\") as f:\n",
    "                    prev_data = json.load(f)\n",
    "            except Exception as e:\n",
    "                self._log(f\"ERROR: Error opening resume file: {str(e)}\")\n",
    "                return False\n",
    "            self.results = prev_data[\"results\"]\n",
    "            self._log(f\"Resuming from {resume_from} holding {len(self.results)} questions\")\n",
    "            for rdict in self.results.values():\n",
    "                if rdict[\"is_correct\"] == True: self.correct_count +=1\n",
    "                self.total_count += 1\n",
    "            self.questions = [q for q in self.questions if q[\"id\"] not in self.results]\n",
    "\n",
    "    def _save_data(self):\n",
    "        \"\"\"Save data to file\"\"\"\n",
    "        data = {\n",
    "            \"subject_id\": self.subject_id,\n",
    "            \"timestamp\": time.time(),\n",
    "            \"accuracy\": self.accuracy,\n",
    "            \"results\": self.results,\n",
    "            \"run_parameters\": self.run_parameters,\n",
    "        }\n",
    "                    \n",
    "        filename = f\"{self.log_base_name}{self.log_suffix}.json\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        self._log(f\"Data saved to: {filename}\")\n",
    "\n",
    "    def _parse_subject_decision(self, subject_answer, options):\n",
    "        \"\"\"Normalize free-form subject answer to a single-letter/choice decision when possible.\"\"\"\n",
    "        if len(subject_answer.rstrip(string.whitespace + string.punctuation)) == 0:\n",
    "            return subject_answer\n",
    "        arr = subject_answer.upper().rstrip(string.whitespace + string.punctuation)\n",
    "        if arr and arr[0] in options:\n",
    "            return arr[0]\n",
    "        if arr and arr[-1] in options:\n",
    "            return arr[-1]\n",
    "        return subject_answer\n",
    "\n",
    "    def _present_question_with_indices(self, question, i, total):\n",
    "        \"\"\"Helper to call present_question with the configured indices.\"\"\"\n",
    "        if self.include_question_num and self.include_total_questions:\n",
    "            return self._present_question(question, i, total)\n",
    "        elif self.include_question_num:\n",
    "            return self._present_question(question, i)\n",
    "        else:\n",
    "            return self._present_question(question)\n",
    "\n",
    "    def _prepare_mc_for_llm(self, question, question_num=None, total_questions=None):\n",
    "        \"\"\"\n",
    "        Prepare MC question text, setup prompt, options list, and (if nested) midpoint map.\n",
    "        Uses present_question indices based on provided question_num/total_questions.\n",
    "        \"\"\"\n",
    "        if self.nested:\n",
    "            q_text = self._present_nested_question(question, self.nested_question_prompt, self.nested_option_dict)\n",
    "            options = list(self.nested_option_dict.keys())\n",
    "            setup_prompt = self.mc_setup_prompt\n",
    "            RANGE_MIDPOINTS = self.nested_range_midpoints\n",
    "        else:\n",
    "            if question_num is None and total_questions is None:\n",
    "                q_text = self._present_question(question)\n",
    "            elif total_questions is None:\n",
    "                q_text = self._present_question(question, question_num)\n",
    "            else:\n",
    "                q_text = self._present_question(question, question_num, total_questions)\n",
    "            options = list(question[\"options\"].keys())\n",
    "            setup_prompt = self.mc_setup_prompt\n",
    "            RANGE_MIDPOINTS = None\n",
    "\n",
    "        options_str = \" or \".join(options) if len(options) == 2 else \", \".join(options[:-1]) + f\", or {options[-1]}\"\n",
    "        llm_prompt = q_text + f\"\\nYour choice ({options_str}): \"\n",
    "        return q_text, setup_prompt, options, RANGE_MIDPOINTS, llm_prompt\n",
    "\n",
    "    def run_capabilities_measurement(self):\n",
    "        \"\"\"\n",
    "        Measures a subject's performance on multiple choice questions.\n",
    "        Uses parallel execution for resampling if configured.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if completed successfully, False otherwise\n",
    "            str: Path to the capabilities data file\n",
    "        \"\"\"\n",
    "        start_message = f\"\\nStarting Capabilities Measurement for Subject: {self.subject_id}\"\n",
    "        self._log(start_message)\n",
    "        self._log(f\"Configuration: Questions={self.n_questions}, is_human_player={self.is_human_player}, temperature={self.temperature}, resample_for_probs={self.resample_for_probs}, nested={self.nested}\")\n",
    "        self._log(\"\\n\" + \"=\"*10 + \" Starting Capability Measuring \" + \"=\"*10)\n",
    "        \n",
    "        log_interval = 10\n",
    "\n",
    "        # This condition diverts the logic to the parallel path\n",
    "        if self.resample_for_probs and not self.is_human_player:\n",
    "            #################################################################\n",
    "            # PARALLEL PATH: For resampling LLM multiple-choice questions\n",
    "            #################################################################\n",
    "            max_workers = 4\n",
    "            epsilon = 0.05\n",
    "            # Record parallel config and fixed prompts\n",
    "            self.run_parameters[\"parallel_config\"] = {\"max_workers\": max_workers, \"epsilon\": epsilon}\n",
    "            self.run_parameters[\"mc_setup_prompt\"] = self.mc_setup_prompt\n",
    "            if self.nested:\n",
    "                self.run_parameters[\"nested_option_dict\"] = self.nested_option_dict\n",
    "                self.run_parameters[\"nested_range_midpoints\"] = self.nested_range_midpoints\n",
    "                self.run_parameters[\"nested_question_prompt\"] = self.nested_question_prompt\n",
    "\n",
    "            # --- Phase 1: Prepare all tasks ---\n",
    "            self._log(f\"Preparing {len(self.questions)} questions for parallel resampling...\")\n",
    "            estimation_tasks = []\n",
    "            total_q = len(self.questions)\n",
    "            for idx, question in enumerate(self.questions, start=1):\n",
    "                _, setup_prompt, options, RANGE_MIDPOINTS, llm_prompt = self._prepare_mc_for_llm(\n",
    "                    question,\n",
    "                    idx if self.include_question_num else None,\n",
    "                    total_q if self.include_total_questions else None\n",
    "                )\n",
    "\n",
    "                task = {\n",
    "                    \"question_obj\": question,\n",
    "                    \"prompt\": setup_prompt + \"\\n\\n\" + llm_prompt,\n",
    "                    \"options\": options,\n",
    "                    \"message_history\": [], # no history\n",
    "                    \"epsilon\": epsilon,\n",
    "                    \"range_midpoints\": RANGE_MIDPOINTS,\n",
    "                }\n",
    "                estimation_tasks.append(task)\n",
    "            \n",
    "            # --- Phase 2: Execute all tasks in parallel ---\n",
    "            parallel_results = self.run_estimations_in_parallel(estimation_tasks, max_workers=max_workers)\n",
    "\n",
    "            # --- Phase 3: Process the results ---\n",
    "            self._log(\"Processing results from parallel execution...\")\n",
    "            for result_item in parallel_results:\n",
    "                if result_item.get('error'):\n",
    "                    self._log(f\"ERROR: Task for question '{result_item['task']['question_obj'].get('id')}' failed: {result_item['error']}\")\n",
    "                    continue\n",
    "                \n",
    "                subject_answer, _, probs = result_item['result']\n",
    "                question = result_item['task']['question_obj']\n",
    "                options = result_item['task']['options']\n",
    "                RANGE_MIDPOINTS = result_item['task'].get('range_midpoints')\n",
    "                \n",
    "                subject_decision = self._parse_subject_decision(subject_answer, options)\n",
    "\n",
    "                if self.nested:\n",
    "                    if probs and RANGE_MIDPOINTS:\n",
    "                        is_correct = sum(\n",
    "                            RANGE_MIDPOINTS[key.strip()] * mass\n",
    "                            for key, mass in probs.items()\n",
    "                            if key.strip() in RANGE_MIDPOINTS\n",
    "                        )\n",
    "                    else:\n",
    "                        is_correct = 0.0\n",
    "                else:\n",
    "                    is_correct = (subject_decision == question[\"correct_answer\"])\n",
    "\n",
    "                if is_correct:\n",
    "                    self.correct_count += 1\n",
    "                \n",
    "                if subject_decision != \"\":\n",
    "                    self.results[question[\"id\"]] = {\n",
    "                        \"question\": question,\n",
    "                        \"subject_answer\": subject_decision,\n",
    "                        \"is_correct\": is_correct,\n",
    "                        \"probs\": probs \n",
    "                    }\n",
    "                self.total_count += 1\n",
    "            \n",
    "            # Save data once at the end of processing\n",
    "            self._save_data()\n",
    "\n",
    "        else:\n",
    "            #################################################################\n",
    "            # SEQUENTIAL PATH: For humans or single-sample runs\n",
    "            #################################################################\n",
    "            probs = None\n",
    "\n",
    "            if self.is_human_player:\n",
    "                # Record human input prompt used\n",
    "                self.run_parameters[\"human_mc_input_prompt\"] = self.human_mc_input_prompt\n",
    "            else:\n",
    "                # Record fixed MC setup and nested settings actually used\n",
    "                self.run_parameters[\"mc_setup_prompt\"] = self.mc_setup_prompt\n",
    "                if self.nested:\n",
    "                    self.run_parameters[\"nested_option_dict\"] = self.nested_option_dict\n",
    "                    self.run_parameters[\"nested_range_midpoints\"] = self.nested_range_midpoints\n",
    "                    self.run_parameters[\"nested_question_prompt\"] = self.nested_question_prompt\n",
    "\n",
    "                # Record static _get_llm_answer args used in this run (MC path)\n",
    "                max_tokens_used = None if ('opus-4' in self.subject_name or 'sonnet-4' in self.subject_name) else 1\n",
    "                self.run_parameters[\"get_llm_answer_static_args\"] = {\n",
    "                    \"keep_appending\": False,\n",
    "                    \"message_history\": [],\n",
    "                    \"MAX_TOKENS\": max_tokens_used,\n",
    "                    \"temp\": self.temperature\n",
    "                }\n",
    "\n",
    "            total_q = len(self.questions)\n",
    "            for i, question in enumerate(self.questions, start=1):\n",
    "                if self.is_human_player:\n",
    "                    # Present once, honoring index config\n",
    "                    q_text = self._present_question_with_indices(question, i, total_q)\n",
    "                    print(q_text)\n",
    "                    subject_answer = self._get_subject_answer(\n",
    "                        list(question[\"options\"].keys()), \n",
    "                        self.human_mc_input_prompt\n",
    "                    )\n",
    "                    if subject_answer is None:\n",
    "                        return False, None\n",
    "                    options = list(question[\"options\"].keys())\n",
    "                    RANGE_MIDPOINTS = None\n",
    "                    probs = None\n",
    "                else:\n",
    "                    # For LLM subject: prepare once, honoring index config\n",
    "                    _, setup_prompt, options, RANGE_MIDPOINTS, llm_prompt = self._prepare_mc_for_llm(\n",
    "                        question,\n",
    "                        i if self.include_question_num else None,\n",
    "                        total_q if self.include_total_questions else None\n",
    "                    )\n",
    "\n",
    "                    gla_args = self.run_parameters[\"get_llm_answer_static_args\"]\n",
    "                    subject_answer, _, probs = self._get_llm_answer(\n",
    "                        options,\n",
    "                        setup_prompt + \"\\n\\n\" + llm_prompt,\n",
    "                        gla_args[\"message_history\"],\n",
    "                        keep_appending=gla_args[\"keep_appending\"],\n",
    "                        MAX_TOKENS=gla_args[\"MAX_TOKENS\"],\n",
    "                        temp=gla_args[\"temp\"]\n",
    "                    )\n",
    "                \n",
    "                # --- Same result processing logic as parallel path ---\n",
    "                subject_decision = self._parse_subject_decision(subject_answer, options)\n",
    "\n",
    "                if self.nested:\n",
    "                    is_correct = (sum(\n",
    "                        RANGE_MIDPOINTS[key.strip()] * mass\n",
    "                        for key, mass in (probs or {}).items()\n",
    "                        if key.strip() in RANGE_MIDPOINTS\n",
    "                    ) if probs else RANGE_MIDPOINTS[subject_decision] if (RANGE_MIDPOINTS and subject_decision in RANGE_MIDPOINTS) else 0.0)\n",
    "                else:\n",
    "                    is_correct = (subject_decision == question[\"correct_answer\"])\n",
    "\n",
    "                if is_correct:\n",
    "                    self.correct_count += 1\n",
    "                \n",
    "                if subject_decision != \"\":\n",
    "                    self.results[question[\"id\"]] = {\n",
    "                        \"question\": question,\n",
    "                        \"subject_answer\": subject_decision,\n",
    "                        \"is_correct\": is_correct,\n",
    "                        \"probs\": probs \n",
    "                    }\n",
    "                self.total_count += 1\n",
    "                print(f\"Completed question {self.total_count}/{len(self.questions)}\")\n",
    "                if (i) % log_interval == 0: self._save_data()\n",
    "        \n",
    "        # --- Finalization steps, common to both paths ---\n",
    "        if self.total_count > 0:\n",
    "            self.accuracy = self.correct_count / self.total_count\n",
    "        else:\n",
    "            self.accuracy = 0.0\n",
    "            self._log(\"WARNING: No questions were processed.\")\n",
    "        \n",
    "        summary = f\"\\nCapabilities Test Complete. Accuracy: {self.accuracy:.2%} ({self.correct_count}/{self.total_count})\"\n",
    "        self._log(summary)\n",
    "        \n",
    "        self._save_data()\n",
    "                    \n",
    "        capabilities_file_path = f\"{self.log_base_name}{self.log_suffix}.json\"\n",
    "        self._log(f\"Capabilities measurement completed. Results saved to: {capabilities_file_path}\")\n",
    "        return True, capabilities_file_path\n",
    "\n",
    "    def run_capabilities_measurement_sa(self):\n",
    "        \"\"\"\n",
    "        This measures a subject's performance on short answer questions and saves the results to a file.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if completed successfully, False otherwise\n",
    "            str: Path to the capabilities data file\n",
    "        \"\"\"\n",
    "        start_message = f\"\\nStarting Capabilities Measurement for Subject: {self.subject_id}\"\n",
    "        self._log(start_message)\n",
    "        self._log(f\"Configuration: Questions={self.n_questions}, is_human_player={self.is_human_player}, temperature={self.temperature}, resample_for_probs={self.resample_for_probs}, nested={self.nested}\")\n",
    "        self._log(\"\\n\" + \"=\"*10 + \" Starting Capability Measuring \" + \"=\"*10)\n",
    "        \n",
    "        # Initialize state\n",
    "        probs = None\n",
    "        log_interval = 10\n",
    "        self.accuracy = None\n",
    "\n",
    "        # Record fixed prompts/args used for this SA run\n",
    "        if self.is_human_player:\n",
    "            self.run_parameters[\"human_sa_input_prompt\"] = self.human_sa_input_prompt\n",
    "        else:\n",
    "            self.run_parameters[\"sa_setup_prompt\"] = self.sa_setup_prompt\n",
    "            self.run_parameters[\"get_llm_answer_static_args\"] = {\n",
    "                \"keep_appending\": False,\n",
    "                \"message_history\": [],\n",
    "                \"MAX_TOKENS\": None,\n",
    "                \"temp\": self.temperature\n",
    "            }\n",
    "        \n",
    "        # Process each question\n",
    "        total_q = len(self.questions)\n",
    "        for i, question in enumerate(self.questions, start=1):\n",
    "            # Present honoring index config\n",
    "            q_text = self._present_question_with_indices(question, i, total_q)\n",
    "\n",
    "            # Get subject's answer\n",
    "            if self.is_human_player:\n",
    "                print(q_text)\n",
    "                subject_answer = self._get_subject_answer(\n",
    "                    [], \n",
    "                    self.human_sa_input_prompt\n",
    "                )\n",
    "                if subject_answer is None:\n",
    "                    return False\n",
    "                probs = None\n",
    "            else:\n",
    "                # For LLM subject\n",
    "                llm_prompt = q_text + \"\\nYour answer: \"\n",
    "                setup_prompt = self.sa_setup_prompt\n",
    "                gla_args = self.run_parameters[\"get_llm_answer_static_args\"]\n",
    "                subject_answer, _, probs = self._get_llm_answer(\n",
    "                    None,\n",
    "                    setup_prompt + \"\\n\\n\" + llm_prompt,\n",
    "                    gla_args[\"message_history\"], # no history\n",
    "                    keep_appending=gla_args[\"keep_appending\"],\n",
    "                    MAX_TOKENS=gla_args[\"MAX_TOKENS\"],\n",
    "                    temp=gla_args[\"temp\"]\n",
    "                )\n",
    "                        \n",
    "            # Store result\n",
    "            if subject_answer != \"\":\n",
    "                self.results[question[\"id\"]] = {\n",
    "                    \"question\": question,\n",
    "                    \"subject_answer\": subject_answer,\n",
    "                    \"is_correct\": None,\n",
    "                    \"probs\": probs \n",
    "                }\n",
    "            self.total_count += 1\n",
    "            print(f\"Completed question {self.total_count}/{len(self.questions)}\")\n",
    "            if (i) % log_interval == 0: self._save_data()\n",
    "            \n",
    "        # Summary\n",
    "        summary = f\"\\nCapabilities Test Complete.\"\n",
    "        self._log(summary)\n",
    "        \n",
    "        self._save_data()\n",
    "                    \n",
    "        # Return the path to the capabilities data file\n",
    "        capabilities_file_path = f\"{self.log_base_name}{self.log_suffix}.json\"\n",
    "        self._log(f\"Capabilities measurement completed. Results saved to: {capabilities_file_path}\")\n",
    "        return True, capabilities_file_path\n",
    "\n",
    "def main(model_dataset_dict, temp):\n",
    "    for subject_name, datasets in model_dataset_dict.items():\n",
    "        for DATASET_NAME in datasets:\n",
    "            IS_HUMAN = False\n",
    "            INCLUDE_QNUM = False\n",
    "            INCLUDE_TOTAL = False\n",
    "            resume_from = None\n",
    "            RESAMPLE = False\n",
    "            NESTED = None #values: None, \"Self\", \"Other\"\n",
    "            temp = temp\n",
    "            seed = 42\n",
    "            \n",
    "            N_QUESTIONS = 5 if IS_HUMAN else 447 if DATASET_NAME.startswith(\"GP\") else 500 \n",
    "            SUBJECT_ID = f\"{subject_name.replace('/', '-')}_{DATASET_NAME}_{N_QUESTIONS}\"\n",
    "            try:\n",
    "                # Load questions for capabilities measurement\n",
    "                print(f\"Loading {N_QUESTIONS} questions for capabilities measurement...\")\n",
    "                formatted_questions = load_and_format_dataset(DATASET_NAME, N_QUESTIONS)\n",
    "\n",
    "                random.seed(seed)\n",
    "                random.shuffle(formatted_questions)\n",
    "                    \n",
    "                if not formatted_questions or len(formatted_questions) < N_QUESTIONS:\n",
    "                    print(f\"Error: Not enough questions available ({len(formatted_questions) if formatted_questions else 0}). Needed: {N_QUESTIONS}\")\n",
    "                    return\n",
    "                \n",
    "                # Create game instance for capabilities measurement\n",
    "                game = CapabilitiesTest(\n",
    "                    subject_id=SUBJECT_ID,\n",
    "                    subject_name=subject_name,\n",
    "                    questions=formatted_questions,\n",
    "                    n_questions=N_QUESTIONS,\n",
    "                    is_human_player=IS_HUMAN,\n",
    "                    resume_from=resume_from,\n",
    "                    temperature=temp,\n",
    "                    resample_for_probs=RESAMPLE,\n",
    "                    nested=NESTED,\n",
    "                    include_question_num=INCLUDE_QNUM,\n",
    "                    include_total_questions=INCLUDE_TOTAL\n",
    "                )\n",
    "\n",
    "                # Store the seed used (run-level, for reproducibility)\n",
    "                game.run_parameters[\"seed\"] = seed\n",
    "                            \n",
    "                # Run capabilities measurement\n",
    "                if (DATASET_NAME == \"SimpleQA\" or DATASET_NAME == \"GPSA\") and not NESTED:\n",
    "                    success, capabilities_file = game.run_capabilities_measurement_sa()\n",
    "                else:\n",
    "                    success, capabilities_file = game.run_capabilities_measurement()\n",
    "                \n",
    "                if success:\n",
    "                    print(f\"\\nCapabilities measurement completed successfully.\")\n",
    "                    print(f\"Results saved to: {capabilities_file}\")\n",
    "                else:\n",
    "                    print(\"\\nCapabilities measurement failed.\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error during execution: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    print(\"\\nExecution completed.\")\n",
    "\n",
    "model_dataset_dict = {\n",
    "    \"qwen3-235b-a22b-2507\": [\"SimpleMC\"],\n",
    "    }\n",
    "main(model_dataset_dict, temp=1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
